{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1 (Request ID: JyxMsgS1C9jZAYi5K4kkU)\n\nRate limit reached. Please log in or use your apiToken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:269\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    270\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/ankush/Projects/aiconfig/python/tests/test.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ankush/Projects/aiconfig/python/tests/test.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m InferenceClient\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ankush/Projects/aiconfig/python/tests/test.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m client \u001b[39m=\u001b[39m InferenceClient(\u001b[39m\"\u001b[39m\u001b[39mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ankush/Projects/aiconfig/python/tests/test.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m client\u001b[39m.\u001b[39;49mtext_generation(\u001b[39m\"\u001b[39;49m\u001b[39mWhat is your favourite condiment?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:1529\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[1;32m   1508\u001b[0m         _set_as_non_tgi(model)\n\u001b[1;32m   1509\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_generation(  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m             prompt\u001b[39m=\u001b[39mprompt,\n\u001b[1;32m   1511\u001b[0m             details\u001b[39m=\u001b[39mdetails,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1527\u001b[0m             decoder_input_details\u001b[39m=\u001b[39mdecoder_input_details,\n\u001b[1;32m   1528\u001b[0m         )\n\u001b[0;32m-> 1529\u001b[0m     raise_text_generation_error(e)\n\u001b[1;32m   1531\u001b[0m \u001b[39m# Parse output\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/inference/_text_generation.py:478\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[0;34m(http_error)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[39mraise\u001b[39;00m exception \u001b[39mfrom\u001b[39;00m \u001b[39mhttp_error\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m \u001b[39mraise\u001b[39;00m http_error\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:1505\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[39m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1505\u001b[0m     bytes_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost(json\u001b[39m=\u001b[39;49mpayload, model\u001b[39m=\u001b[39;49mmodel, task\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m, stream\u001b[39m=\u001b[39;49mstream)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1507\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, BadRequestError) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:238\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m InferenceTimeoutError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInference call timed out: \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror\u001b[39;00m  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     hf_raise_for_status(response)\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39miter_lines() \u001b[39mif\u001b[39;00m stream \u001b[39melse\u001b[39;00m response\u001b[39m.\u001b[39mcontent\n\u001b[1;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:320\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(message, response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m \u001b[39mraise\u001b[39;00m HfHubHTTPError(\u001b[39mstr\u001b[39m(e), response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1 (Request ID: JyxMsgS1C9jZAYi5K4kkU)\n\nRate limit reached. Please log in or use your apiToken"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "client.text_generation(\"What is your favourite condiment?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig import AIConfigRuntime\n",
    "from aiconfig import Prompt\n",
    "from aiconfig.AIConfigSettings import ModelMetadata, PromptMetadata\n",
    "from aiconfig.default_parsers.hf import HuggingFaceTextParser\n",
    "\n",
    "x = AIConfigRuntime.from_config(\"aiconfigs/basic_chatgpt_query_config.json\")\n",
    "\n",
    "HF_parser = HuggingFaceTextParser(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "x.register_model_parser(HF_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1 (Request ID: G5cCzOF8ape0-X4jzVUXl)\n\nRate limit reached. Please log in or use your apiToken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:269\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    270\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/ankush/Projects/aiconfig/python/tests/test.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ankush/Projects/aiconfig/python/tests/test.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m openai\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msk-atZztDlb06LNZWthLh56T3BlbkFJFd1rQge0yF4OQcCXvqLH\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ankush/Projects/aiconfig/python/tests/test.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m inference_options \u001b[39m=\u001b[39m InferenceOptions()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ankush/Projects/aiconfig/python/tests/test.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mawait\u001b[39;00m x\u001b[39m.\u001b[39mrun(\u001b[39m\"\u001b[39m\u001b[39mprompt2\u001b[39m\u001b[39m\"\u001b[39m, {})\n",
      "File \u001b[0;32m~/Projects/aiconfig/python/src/aiconfig/Config.py:199\u001b[0m, in \u001b[0;36mAIConfigRuntime.run\u001b[0;34m(self, prompt_name, params, options, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m model_name \u001b[39m=\u001b[39m prompt_data\u001b[39m.\u001b[39mget_model_name()\n\u001b[1;32m    197\u001b[0m model_provider \u001b[39m=\u001b[39m AIConfigRuntime\u001b[39m.\u001b[39mget_model_parser(model_name)\n\u001b[0;32m--> 199\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m model_provider\u001b[39m.\u001b[39mrun(prompt_data, \u001b[39mself\u001b[39m, options, params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Projects/aiconfig/python/src/aiconfig/default_parsers/parameterized_model_parser.py:52\u001b[0m, in \u001b[0;36mParameterizedModelParser.run\u001b[0;34m(self, prompt, aiconfig, options, parameters, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_with_dependencies(prompt, aiconfig, options, parameters)\n\u001b[1;32m     51\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_inference(prompt, aiconfig, options, parameters)\n",
      "File \u001b[0;32m~/Projects/aiconfig/python/src/aiconfig/default_parsers/hf.py:131\u001b[0m, in \u001b[0;36mHuggingFaceTextParser.run_inference\u001b[0;34m(self, prompt, aiconfig, options, parameters)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39m# if stream enabled in runtime options and config, then stream. Otherwise don't stream.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m stream \u001b[39m=\u001b[39m (options\u001b[39m.\u001b[39mstream \u001b[39mif\u001b[39;00m options \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    128\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m completion_data \u001b[39mand\u001b[39;00m completion_data\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    129\u001b[0m )\n\u001b[0;32m--> 131\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mtext_generation(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompletion_data)\n\u001b[1;32m    132\u001b[0m response_is_detailed \u001b[39m=\u001b[39m completion_data\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdetails\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m outputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:1529\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[1;32m   1508\u001b[0m         _set_as_non_tgi(model)\n\u001b[1;32m   1509\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_generation(  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m             prompt\u001b[39m=\u001b[39mprompt,\n\u001b[1;32m   1511\u001b[0m             details\u001b[39m=\u001b[39mdetails,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1527\u001b[0m             decoder_input_details\u001b[39m=\u001b[39mdecoder_input_details,\n\u001b[1;32m   1528\u001b[0m         )\n\u001b[0;32m-> 1529\u001b[0m     raise_text_generation_error(e)\n\u001b[1;32m   1531\u001b[0m \u001b[39m# Parse output\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/inference/_text_generation.py:478\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[0;34m(http_error)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[39mraise\u001b[39;00m exception \u001b[39mfrom\u001b[39;00m \u001b[39mhttp_error\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m \u001b[39mraise\u001b[39;00m http_error\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:1505\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[39m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1505\u001b[0m     bytes_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost(json\u001b[39m=\u001b[39;49mpayload, model\u001b[39m=\u001b[39;49mmodel, task\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m, stream\u001b[39m=\u001b[39;49mstream)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1507\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, BadRequestError) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:238\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m InferenceTimeoutError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInference call timed out: \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror\u001b[39;00m  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     hf_raise_for_status(response)\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39miter_lines() \u001b[39mif\u001b[39;00m stream \u001b[39melse\u001b[39;00m response\u001b[39m.\u001b[39mcontent\n\u001b[1;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:320\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(message, response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m \u001b[39mraise\u001b[39;00m HfHubHTTPError(\u001b[39mstr\u001b[39m(e), response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1 (Request ID: G5cCzOF8ape0-X4jzVUXl)\n\nRate limit reached. Please log in or use your apiToken"
     ]
    }
   ],
   "source": [
    "from aiconfig.model_parser import InferenceOptions\n",
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "inference_options = InferenceOptions()\n",
    "await x.run(\"prompt2\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1 (Request ID: jA7liBCVvjhbKr1rSx9C7)\n\nRate limit reached. Please log in or use your apiToken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:269\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    270\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/ankush/Projects/aiconfig/python/tests/test.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ankush/Projects/aiconfig/python/tests/test.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mawait\u001b[39;00m x\u001b[39m.\u001b[39mrun(\u001b[39m\"\u001b[39m\u001b[39mprompt2\u001b[39m\u001b[39m\"\u001b[39m, {})\n",
      "File \u001b[0;32m~/Projects/aiconfig/python/src/aiconfig/Config.py:199\u001b[0m, in \u001b[0;36mAIConfigRuntime.run\u001b[0;34m(self, prompt_name, params, options, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m model_name \u001b[39m=\u001b[39m prompt_data\u001b[39m.\u001b[39mget_model_name()\n\u001b[1;32m    197\u001b[0m model_provider \u001b[39m=\u001b[39m AIConfigRuntime\u001b[39m.\u001b[39mget_model_parser(model_name)\n\u001b[0;32m--> 199\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m model_provider\u001b[39m.\u001b[39mrun(prompt_data, \u001b[39mself\u001b[39m, options, params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Projects/aiconfig/python/src/aiconfig/default_parsers/parameterized_model_parser.py:52\u001b[0m, in \u001b[0;36mParameterizedModelParser.run\u001b[0;34m(self, prompt, aiconfig, options, parameters, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_with_dependencies(prompt, aiconfig, options, parameters)\n\u001b[1;32m     51\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_inference(prompt, aiconfig, options, parameters)\n",
      "File \u001b[0;32m~/Projects/aiconfig/python/src/aiconfig/default_parsers/hf.py:131\u001b[0m, in \u001b[0;36mHuggingFaceTextParser.run_inference\u001b[0;34m(self, prompt, aiconfig, options, parameters)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39m# if stream enabled in runtime options and config, then stream. Otherwise don't stream.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m stream \u001b[39m=\u001b[39m (options\u001b[39m.\u001b[39mstream \u001b[39mif\u001b[39;00m options \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    128\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m completion_data \u001b[39mand\u001b[39;00m completion_data\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    129\u001b[0m )\n\u001b[0;32m--> 131\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mtext_generation(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompletion_data)\n\u001b[1;32m    132\u001b[0m response_is_detailed \u001b[39m=\u001b[39m completion_data\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdetails\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m outputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:1529\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[1;32m   1508\u001b[0m         _set_as_non_tgi(model)\n\u001b[1;32m   1509\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_generation(  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m             prompt\u001b[39m=\u001b[39mprompt,\n\u001b[1;32m   1511\u001b[0m             details\u001b[39m=\u001b[39mdetails,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1527\u001b[0m             decoder_input_details\u001b[39m=\u001b[39mdecoder_input_details,\n\u001b[1;32m   1528\u001b[0m         )\n\u001b[0;32m-> 1529\u001b[0m     raise_text_generation_error(e)\n\u001b[1;32m   1531\u001b[0m \u001b[39m# Parse output\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/inference/_text_generation.py:478\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[0;34m(http_error)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[39mraise\u001b[39;00m exception \u001b[39mfrom\u001b[39;00m \u001b[39mhttp_error\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m \u001b[39mraise\u001b[39;00m http_error\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:1505\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[39m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1505\u001b[0m     bytes_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost(json\u001b[39m=\u001b[39;49mpayload, model\u001b[39m=\u001b[39;49mmodel, task\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m, stream\u001b[39m=\u001b[39;49mstream)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1507\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, BadRequestError) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:238\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m InferenceTimeoutError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInference call timed out: \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror\u001b[39;00m  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     hf_raise_for_status(response)\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39miter_lines() \u001b[39mif\u001b[39;00m stream \u001b[39melse\u001b[39;00m response\u001b[39m.\u001b[39mcontent\n\u001b[1;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:320\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(message, response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m \u001b[39mraise\u001b[39;00m HfHubHTTPError(\u001b[39mstr\u001b[39m(e), response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1 (Request ID: jA7liBCVvjhbKr1rSx9C7)\n\nRate limit reached. Please log in or use your apiToken"
     ]
    }
   ],
   "source": [
    "await x.run(\"prompt2\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prompt(name='prompt1', input='Hi! Tell me 10 cool things to do in NYC.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={}), tags=None, parameters={}, remember_chat_context=True), outputs=[]),\n",
       " Prompt(name='prompt2', input='Hi! Tell me 10 cool things to do in NYC.', metadata=PromptMetadata(model=ModelMetadata(name='mistralai/Mistral-7B-Instruct-v0.1', settings={'stream': True, 'details': True}), tags=None, parameters={}), outputs=[]),\n",
       " Prompt(name='prompt3', input='Evaluate the following texts. Tell me which one is better. \\n\\n Text one: {{prompt1.output}} \\n\\n Text Two: {{prompt2.output}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={}), tags=None, parameters={}, remember_chat_context=False), outputs=[])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.prompt_index.get(\"prompt2\").metadata.model.settings[\"details\"] = True\n",
    "x.prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextGenerationStreamResponse(token=Token(id=13, text='\\n', logprob=-0.7290039, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=13, text='\\n', logprob=-0.39160156, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=5183, text='My', logprob=-0.78564453, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=16020, text=' favourite', logprob=-0.23046875, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=2076, text=' cond', logprob=-0.0019950867, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=2487, text='iment', logprob=-0.0003643036, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=349, text=' is', logprob=-0.28808594, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=446, text=' k', logprob=-1.4121094, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=4455, text='etch', logprob=-0.001660347, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=715, text='up', logprob=-6.914139e-06, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=28723, text='.', logprob=-0.22937012, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=661, text=' It', logprob=-0.60839844, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=28742, text=\"'\", logprob=-0.78564453, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=28713, text='s', logprob=-2.503395e-06, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=3502, text=' vers', logprob=-0.24609375, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=13491, text='atile', logprob=-3.2663345e-05, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=28725, text=',', logprob=-0.65478516, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=261, text=' t', logprob=-0.7421875, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=11136, text='asty', logprob=-4.2319298e-05, special=False), generated_text=None, details=None)\n",
      "TextGenerationStreamResponse(token=Token(id=28725, text=',', logprob=-0.6040039, special=False), generated_text=\"\\n\\nMy favourite condiment is ketchup. It's versatile, tasty,\", details=StreamDetails(finish_reason=<FinishReason.Length: 'length'>, generated_tokens=20, seed=None))\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "r = client.text_generation(\"What is your favourite condiment?\", details = True, stream=True)\n",
    "from huggingface_hub.inference._text_generation import (\n",
    "    TextGenerationParameters,\n",
    "    TextGenerationRequest,\n",
    "    TextGenerationResponse,\n",
    "    TextGenerationStreamResponse,\n",
    "    raise_text_generation_error,\n",
    ")\n",
    "for iteration in r:\n",
    "    print(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(id=28725, text=',', logprob=-0.6040039, special=False) StreamDetails(finish_reason=<FinishReason.Length: 'length'>, generated_tokens=20, seed=None)\n"
     ]
    }
   ],
   "source": [
    "print(iteration.token, iteration.details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
