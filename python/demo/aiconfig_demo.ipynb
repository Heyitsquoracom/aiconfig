{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 install the package\n",
    "\n",
    "package_path = \" ..\"\n",
    "requirements = \"../requirements.txt\"\n",
    "\n",
    "!python3 -m pip install -r $requirements\n",
    "!python3 -m pip install -e $package_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample AI Config Representation\n",
    "```json\n",
    "{\n",
    "    \"name\": \"gpt4 as your data engineer\",\n",
    "    \"description\": \"\",\n",
    "    \"schema_version\": \"latest\",\n",
    "    \"metadata\": {\n",
    "        \"parameters\": {},\n",
    "        \"models\": {\n",
    "            \"gpt-3.5-turbo\": {\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"top_p\": 1,\n",
    "                \"max_tokens\": 3000,\n",
    "                \"temperature\": 1\n",
    "            },\n",
    "            \"gpt-4\": {\n",
    "                \"model\": \"gpt-4\",\n",
    "                \"top_p\": 1,\n",
    "                \"max_tokens\": 3000,\n",
    "                \"temperature\": 1,\n",
    "                \"system_prompt\": \"You are an expert at SQL. You will output nicely formatted SQL code with labels on columns. You will provide a short 1-2 sentence summary on the code. Name columns as one word using underscore and lowercase. Format Output in markdown ### SQL Query code block with SQL Query &nbsp; ### Summary short summary on code\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"prompts\": [\n",
    "        {\n",
    "            \"name\": \"prompt1\",\n",
    "            \"input\": \"Write me a {{sql_language}} query to get this final output: {{output_data}}. Use the tables relationships defined here: {{table_relationships}}.\",\n",
    "            \"metadata\": {\n",
    "                \"model\": {\n",
    "                    \"name\": \"gpt-3.5-turbo\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"prompt2\",\n",
    "            \"input\": \"Translate the following into PostgreSQL code:\\n {{prompt1.output}}\",\n",
    "            \"metadata\": {\n",
    "                \"model\": {\n",
    "                    \"name\": \"gpt-4\",\n",
    "                    \"settings\": {\n",
    "                        \"model\": \"gpt-4\",\n",
    "                        \"top_p\": 1,\n",
    "                        \"max_tokens\": 3000,\n",
    "                        \"temperature\": 1\n",
    "                        \n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load A Config\n",
    "\n",
    "from aiconfig_tools import AIConfigRuntime\n",
    "\n",
    "config_file_path = \"parametrized_data_config.json\"\n",
    "config = AIConfigRuntime.from_config(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt1 takes in 3 parameters: sql_language, output_data, table_relationships\n",
    "# Lets define some parameter values for prompt1\n",
    "\n",
    "prompt1_params = {\n",
    "    \"sql_language\": \"mysql\",\n",
    "    \"output_data\": \"user_name, user_email, trial, num_trial_steps, num_trial_steps_params. output granularity is the trial_id.\",\n",
    "    \"table_relationships\": \"user table, trial table, trial_step table. a user can create many trials. each trial has many trial_steps. a trial_step has parameters if metadata[0] (json) has a non-null parameters value. \"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if I want to see the completion parameters of a prompt before inference? This is useful for debugging, calculating cost of api calls, etc.\n",
    "\n",
    "%env OPENAI_API_KEY=0\n",
    "prompt1_completion_params = await config.resolve(\"prompt1\", prompt1_params) \n",
    "\n",
    "import pprint\n",
    "# Lets take a look at the completion parameters\n",
    "pprint.pprint(prompt1_completion_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great! Now lets run prompt1 with the above parameters. Make sure to have your api credentials set.\n",
    "import asyncio\n",
    "\n",
    "prompt1_output = await config.run(prompt1_params, \"prompt1\")\n",
    "print(prompt1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming\n",
    "\n",
    "from aiconfig_tools import InferenceOptions\n",
    "\n",
    "# Lets define a handler for streaming OpenAI API calls. This function will be called for every delta in the output stream.\n",
    "def print_stream_delta(data, accumulated_data, index: int):\n",
    "    \"\"\"\n",
    "    streamCallback function that prints the stream output to console.\n",
    "    \"\"\"\n",
    "    if \"content\" in data:\n",
    "        content = data['content']\n",
    "        print(content, end = \"\", flush=True)\n",
    "\n",
    "\n",
    "# Define inference options to stream the output of the API call.\n",
    "inference_options = InferenceOptions(stream_callback=print_stream_delta, stream=True)\n",
    "\n",
    "# Run with streaming enabled, as well as the other parameters we defined earlier.\n",
    "await config.run(prompt1_params, \"prompt1\", inference_options) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chained Parameterized Prompts\n",
    "\n",
    "# prompt 2 uses prompt1's output. Prompt2 asks to translate into postgres\n",
    "await config.run({}, \"prompt2\", inference_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRUD API\n",
    "# Programatically add, delete, and update prompts in the config.\n",
    "import json\n",
    "from aiconfig_tools.AIConfigSettings import Prompt\n",
    "\n",
    "# Lets define a new prompt\n",
    "# Chat History defaults to True. Lets explicitly set it to false.\n",
    "prompt = Prompt(**{\"name\": \"prompt3\", \n",
    "                   \"input\": \"How do transformers work?\", \n",
    "                   \"metadata\": {\n",
    "                       \"model\": \"gpt-3.5-turbo\",\n",
    "                       \"remember_chat_context\": False,\n",
    "                }})\n",
    "\n",
    "config.delete_prompt(\"prompt3\")\n",
    "config.add_prompt(\"prompt3\", prompt)\n",
    "\n",
    "# Exports aiconfig to disk.\n",
    "config.save(\"updated_aiconfig.json\")\n",
    "\n",
    "# Let's check out the new update config\n",
    "\n",
    "with open('updated_aiconfig.json', 'r') as f:\n",
    "    parsed = json.load(f)\n",
    "    pprint.pprint(parsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = AIConfigRuntime.from_config(\"updated_aiconfig.json\")\n",
    "\n",
    "print(await config2.resolve( \"prompt3\",{}, inference_options))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
