{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: black in /opt/homebrew/lib/python3.11/site-packages (from -r ../requirements.txt (line 1)) (23.9.1)\n",
      "Requirement already satisfied: flake8 in /opt/homebrew/lib/python3.11/site-packages (from -r ../requirements.txt (line 2)) (6.1.0)\n",
      "Requirement already satisfied: pytest in /opt/homebrew/lib/python3.11/site-packages (from -r ../requirements.txt (line 3)) (7.4.2)\n",
      "Requirement already satisfied: pydantic>=2.1 in /opt/homebrew/lib/python3.11/site-packages (from -r ../requirements.txt (line 4)) (2.4.2)\n",
      "Requirement already satisfied: pybars3 in /opt/homebrew/lib/python3.11/site-packages (from -r ../requirements.txt (line 5)) (0.9.7)\n",
      "Requirement already satisfied: google-generativeai in /opt/homebrew/lib/python3.11/site-packages (from -r ../requirements.txt (line 6)) (0.2.1)\n",
      "Requirement already satisfied: openai in /opt/homebrew/lib/python3.11/site-packages (from -r ../requirements.txt (line 7)) (0.28.1)\n",
      "Collecting python-dotenv (from -r ../requirements.txt (line 8))\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/lib/python3.11/site-packages (from black->-r ../requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /opt/homebrew/lib/python3.11/site-packages (from black->-r ../requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/homebrew/lib/python3.11/site-packages (from black->-r ../requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /opt/homebrew/lib/python3.11/site-packages (from black->-r ../requirements.txt (line 1)) (0.11.2)\n",
      "Requirement already satisfied: platformdirs>=2 in /Users/saqadri/Library/Python/3.11/lib/python/site-packages (from black->-r ../requirements.txt (line 1)) (3.11.0)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from flake8->-r ../requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.12.0,>=2.11.0 in /opt/homebrew/lib/python3.11/site-packages (from flake8->-r ../requirements.txt (line 2)) (2.11.0)\n",
      "Requirement already satisfied: pyflakes<3.2.0,>=3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from flake8->-r ../requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: iniconfig in /opt/homebrew/lib/python3.11/site-packages (from pytest->-r ../requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/homebrew/lib/python3.11/site-packages (from pytest->-r ../requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=2.1->-r ../requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=2.1->-r ../requirements.txt (line 4)) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=2.1->-r ../requirements.txt (line 4)) (4.8.0)\n",
      "Requirement already satisfied: PyMeta3>=0.5.1 in /opt/homebrew/lib/python3.11/site-packages (from pybars3->-r ../requirements.txt (line 5)) (0.5.1)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.3.3 in /opt/homebrew/lib/python3.11/site-packages (from google-generativeai->-r ../requirements.txt (line 6)) (0.3.3)\n",
      "Requirement already satisfied: google-auth in /opt/homebrew/lib/python3.11/site-packages (from google-generativeai->-r ../requirements.txt (line 6)) (2.23.3)\n",
      "Requirement already satisfied: google-api-core in /opt/homebrew/lib/python3.11/site-packages (from google-generativeai->-r ../requirements.txt (line 6)) (2.12.0)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/lib/python3.11/site-packages (from google-generativeai->-r ../requirements.txt (line 6)) (4.24.4)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from google-generativeai->-r ../requirements.txt (line 6)) (4.66.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/homebrew/lib/python3.11/site-packages (from google-ai-generativelanguage==0.3.3->google-generativeai->-r ../requirements.txt (line 6)) (1.22.3)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/homebrew/lib/python3.11/site-packages (from openai->-r ../requirements.txt (line 7)) (2.31.0)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.11/site-packages (from openai->-r ../requirements.txt (line 7)) (3.8.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.20->openai->-r ../requirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.20->openai->-r ../requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.20->openai->-r ../requirements.txt (line 7)) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.20->openai->-r ../requirements.txt (line 7)) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->openai->-r ../requirements.txt (line 7)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->openai->-r ../requirements.txt (line 7)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->openai->-r ../requirements.txt (line 7)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->openai->-r ../requirements.txt (line 7)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->openai->-r ../requirements.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->openai->-r ../requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/homebrew/lib/python3.11/site-packages (from google-api-core->google-generativeai->-r ../requirements.txt (line 6)) (1.60.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth->google-generativeai->-r ../requirements.txt (line 6)) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from google-auth->google-generativeai->-r ../requirements.txt (line 6)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/lib/python3.11/site-packages (from google-auth->google-generativeai->-r ../requirements.txt (line 6)) (4.9)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/homebrew/lib/python3.11/site-packages (from google-api-core->google-generativeai->-r ../requirements.txt (line 6)) (1.59.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/homebrew/lib/python3.11/site-packages (from google-api-core->google-generativeai->-r ../requirements.txt (line 6)) (1.59.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/homebrew/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth->google-generativeai->-r ../requirements.txt (line 6)) (0.5.0)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n",
      "Obtaining file:///Users/saqadri/lm/aiconfig/python\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: aiconfig-tools\n",
      "  Building editable for aiconfig-tools (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aiconfig-tools: filename=aiconfig_tools-0.0.0-0.editable-py3-none-any.whl size=1614 sha256=586d32f164b4a2054110d043001330f0a6e01269cfca21660695756bd3cbf9ff\n",
      "  Stored in directory: /private/var/folders/mk/5l10lyqs1c73_pj2grj9f9vw0000gn/T/pip-ephem-wheel-cache-wbemwqr3/wheels/34/b7/3a/3b195cd5a37451352873f443051045b67ab94bc894255dfc92\n",
      "Successfully built aiconfig-tools\n",
      "Installing collected packages: aiconfig-tools\n",
      "  Attempting uninstall: aiconfig-tools\n",
      "    Found existing installation: aiconfig-tools 0.0.0\n",
      "    Uninstalling aiconfig-tools-0.0.0:\n",
      "      Successfully uninstalled aiconfig-tools-0.0.0\n",
      "Successfully installed aiconfig-tools-0.0.0\n"
     ]
    }
   ],
   "source": [
    "#1 install the package\n",
    "\n",
    "package_path = \" ..\"\n",
    "requirements = \"../requirements.txt\"\n",
    "\n",
    "!python3 -m pip install -r $requirements\n",
    "!python3 -m pip install -e $package_path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample AI Config Representation\n",
    "```json\n",
    "{\n",
    "    \"name\": \"gpt4 as your data engineer\",\n",
    "    \"description\": \"\",\n",
    "    \"schema_version\": \"latest\",\n",
    "    \"metadata\": {\n",
    "        \"parameters\": {},\n",
    "        \"models\": {\n",
    "            \"gpt-3.5-turbo\": {\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"top_p\": 1,\n",
    "                \"max_tokens\": 3000,\n",
    "                \"temperature\": 1\n",
    "            },\n",
    "            \"gpt-4\": {\n",
    "                \"model\": \"gpt-4\",\n",
    "                \"top_p\": 1,\n",
    "                \"max_tokens\": 3000,\n",
    "                \"temperature\": 1,\n",
    "                \"system_prompt\": \"You are an expert at SQL. You will output nicely formatted SQL code with labels on columns. You will provide a short 1-2 sentence summary on the code. Name columns as one word using underscore and lowercase. Format Output in markdown ### SQL Query code block with SQL Query &nbsp; ### Summary short summary on code\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"prompts\": [\n",
    "        {\n",
    "            \"name\": \"prompt1\",\n",
    "            \"input\": \"Write me a {{sql_language}} query to get this final output: {{output_data}}. Use the tables relationships defined here: {{table_relationships}}.\",\n",
    "            \"metadata\": {\n",
    "                \"model\": {\n",
    "                    \"name\": \"gpt-3.5-turbo\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"prompt2\",\n",
    "            \"input\": \"Translate the following into PostgreSQL code:\\n {{prompt1.output}}\",\n",
    "            \"metadata\": {\n",
    "                \"model\": {\n",
    "                    \"name\": \"gpt-4\",\n",
    "                    \"settings\": {\n",
    "                        \"model\": \"gpt-4\",\n",
    "                        \"top_p\": 1,\n",
    "                        \"max_tokens\": 3000,\n",
    "                        \"temperature\": 1\n",
    "                        \n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load A Config\n",
    "\n",
    "from aiconfig_tools import AIConfigRuntime\n",
    "\n",
    "config_file_path = \"parametrized_data_config.json\"\n",
    "config = AIConfigRuntime.from_config(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt1 takes in 3 parameters: sql_language, output_data, table_relationships\n",
    "# Lets define some parameter values for prompt1\n",
    "\n",
    "prompt1_params = {\n",
    "    \"sql_language\": \"mysql\",\n",
    "    \"output_data\": \"user_name, user_email, trial, num_trial_steps, num_trial_steps_params. output granularity is the trial_id.\",\n",
    "    \"table_relationships\": \"user table, trial table, trial_step table. a user can create many trials. each trial has many trial_steps. a trial_step has parameters if metadata[0] (json) has a non-null parameters value. \"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_tokens': 3000,\n",
      " 'messages': [{'content': 'Write me a mysql query to get this final output: '\n",
      "                          'user_name, user_email, trial, num_trial_steps, '\n",
      "                          'num_trial_steps_params. output granularity is the '\n",
      "                          'trial_id.. Use the tables relationships defined '\n",
      "                          'here: user table, trial table, trial_step table. a '\n",
      "                          'user can create many trials. each trial has many '\n",
      "                          'trial_steps. a trial_step has parameters if '\n",
      "                          'metadata[0] (json) has a non-null parameters value. '\n",
      "                          '.',\n",
      "               'role': 'user'}],\n",
      " 'model': 'gpt-3.5-turbo',\n",
      " 'temperature': 1,\n",
      " 'top_p': 1}\n"
     ]
    }
   ],
   "source": [
    "# What if I want to see the completion parameters of a prompt before inference? This is useful for debugging, calculating cost of api calls, etc.\n",
    "\n",
    "prompt1_completion_params = await config.resolve(\"prompt1\", prompt1_params) \n",
    "\n",
    "import pprint\n",
    "# Lets take a look at the completion parameters\n",
    "pprint.pprint(prompt1_completion_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ExecuteResult(output_type='execute_result', execution_count=0.0, data=<OpenAIObject at 0x11c4d3b30> JSON: {\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"Here is the MySQL query to retrieve the desired output:\\n\\n```sql\\nSELECT u.user_name, u.user_email, t.trial_id AS trial, COUNT(ts.trial_step_id) AS num_trial_steps, SUM(CASE WHEN ts.metadata -> '$[0].parameters' IS NOT NULL THEN 1 ELSE 0 END) AS num_trial_steps_params \\nFROM user u\\nJOIN trial t ON u.user_id = t.user_id\\nJOIN trial_step ts ON t.trial_id = ts.trial_id\\nGROUP BY u.user_name, u.user_email, t.trial_id;\\n```\\n\\nThis query joins the `user`, `trial`, and `trial_step` tables using their relationships. We then group the result by `user_name`, `user_email`, and `trial_id` to get the desired granularity. The `COUNT` function is used to count the total number of trial steps for each trial, and the `SUM` function with a `CASE` statement is used to count the number of trial steps with non-null parameters. The output columns are `user_name`, `user_email`, `trial`, `num_trial_steps`, and `num_trial_steps_params`.\"\n",
      "}, metadata={'id': 'chatcmpl-89GR5kDALPrZ1AIalbvA139XSrCac', 'object': 'chat.completion', 'created': 1697218823, 'model': 'gpt-3.5-turbo-0613', 'usage': <OpenAIObject at 0x11c4d3d10> JSON: {\n",
      "  \"prompt_tokens\": 94,\n",
      "  \"completion_tokens\": 237,\n",
      "  \"total_tokens\": 331\n",
      "}, 'finish_reason': 'stop'})]\n"
     ]
    }
   ],
   "source": [
    "# Great! Now lets run prompt1 with the above parameters. Make sure to have your api credentials set.\n",
    "import asyncio\n",
    "\n",
    "prompt1_output = await config.run(\"prompt1\", prompt1_params)\n",
    "print(prompt1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To generate the desired output, you can use the following MySQL query:\n",
      "\n",
      "```sql\n",
      "SELECT\n",
      "    u.user_name,\n",
      "    u.user_email,\n",
      "    t.trial_id,\n",
      "    COUNT(ts.trial_step_id) AS num_trial_steps,\n",
      "    SUM(CASE WHEN JSON_EXTRACT(ts.metadata, '$[0].parameters') IS NOT NULL THEN 1 ELSE 0 END) AS num_trial_steps_params\n",
      "FROM\n",
      "    user u\n",
      "    INNER JOIN trial t ON u.user_id = t.user_id\n",
      "    LEFT JOIN trial_step ts ON t.trial_id = ts.trial_id\n",
      "GROUP BY\n",
      "    u.user_id,\n",
      "    t.trial_id;\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "1. We select the necessary columns: user_name, user_email, trial_id, and two calculated columns for num_trial_steps and num_trial_steps_params.\n",
      "2. We join the user table with the trial table using the user_id as the common column.\n",
      "3. We then left join the trial_step table with the trial table using the trial_id as the common column.\n",
      "4. In the SELECT clause, we use the COUNT() function to count the number of trial_steps per trial_id.\n",
      "5. We use the JSON_EXTRACT function to extract the parameters value from the metadata JSON array. If the parameters value is not null, we assign 1; otherwise, we assign 0. We then sum these values using the SUM() function to get the count of trial_steps with parameters per trial_id.\n",
      "6. Finally, we group the result by the user_id and trial_id to get the desired granularity.\n",
      "\n",
      "Please note that you will need to adjust the table and column names according to your database schema."
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ExecuteResult(output_type='execute_result', execution_count=0.0, data={'role': 'assistant', 'content': \"To generate the desired output, you can use the following MySQL query:\\n\\n```sql\\nSELECT\\n    u.user_name,\\n    u.user_email,\\n    t.trial_id,\\n    COUNT(ts.trial_step_id) AS num_trial_steps,\\n    SUM(CASE WHEN JSON_EXTRACT(ts.metadata, '$[0].parameters') IS NOT NULL THEN 1 ELSE 0 END) AS num_trial_steps_params\\nFROM\\n    user u\\n    INNER JOIN trial t ON u.user_id = t.user_id\\n    LEFT JOIN trial_step ts ON t.trial_id = ts.trial_id\\nGROUP BY\\n    u.user_id,\\n    t.trial_id;\\n```\\n\\nExplanation:\\n1. We select the necessary columns: user_name, user_email, trial_id, and two calculated columns for num_trial_steps and num_trial_steps_params.\\n2. We join the user table with the trial table using the user_id as the common column.\\n3. We then left join the trial_step table with the trial table using the trial_id as the common column.\\n4. In the SELECT clause, we use the COUNT() function to count the number of trial_steps per trial_id.\\n5. We use the JSON_EXTRACT function to extract the parameters value from the metadata JSON array. If the parameters value is not null, we assign 1; otherwise, we assign 0. We then sum these values using the SUM() function to get the count of trial_steps with parameters per trial_id.\\n6. Finally, we group the result by the user_id and trial_id to get the desired granularity.\\n\\nPlease note that you will need to adjust the table and column names according to your database schema.\"}, metadata={'finish_reason': 'stop'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Streaming\n",
    "\n",
    "from aiconfig_tools import InferenceOptions\n",
    "\n",
    "# Lets define a handler for streaming OpenAI API calls. This function will be called for every delta in the output stream.\n",
    "def print_stream_delta(data, accumulated_data, index: int):\n",
    "    \"\"\"\n",
    "    streamCallback function that prints the stream output to console.\n",
    "    \"\"\"\n",
    "    if \"content\" in data:\n",
    "        content = data['content']\n",
    "        print(content, end = \"\", flush=True)\n",
    "\n",
    "\n",
    "# Define inference options to stream the output of the API call.\n",
    "inference_options = InferenceOptions(stream_callback=print_stream_delta, stream=True)\n",
    "\n",
    "# Run with streaming enabled, as well as the other parameters we defined earlier.\n",
    "await config.run(\"prompt1\", prompt1_params, inference_options) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SQL Query\n",
      "\n",
      "```SQL\n",
      "SELECT\n",
      "    u.user_name as username,\n",
      "    u.user_email as useremail,\n",
      "    t.trial_id as trialid,\n",
      "    COUNT(ts.trial_step_id) AS num_trial_steps,\n",
      "    SUM(CASE WHEN (ts.metadata->'parameters') IS NOT NULL THEN 1 ELSE 0 END) AS num_trial_steps_params\n",
      "FROM\n",
      "    user u\n",
      "    INNER JOIN trial t ON u.user_id = t.user_id\n",
      "    LEFT JOIN trial_step ts ON t.trial_id = ts.trial_id\n",
      "GROUP BY\n",
      "    u.user_id,\n",
      "    t.trial_id;\n",
      "```\n",
      "&nbsp;\n",
      "### Summary \n",
      "\n",
      "The provided PostgreSQL query retrieves the username, user email and trial ID from the database. Also, it calculates the total number of trial steps and the count of trial steps that have parameters for each user and trial. The involvement of three tables in the query: user, trial and trial_step necessitates their join operations. The data is then grouped by user and trial ID."
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ExecuteResult(output_type='execute_result', execution_count=0.0, data={'role': 'assistant', 'content': \"### SQL Query\\n\\n```SQL\\nSELECT\\n    u.user_name as username,\\n    u.user_email as useremail,\\n    t.trial_id as trialid,\\n    COUNT(ts.trial_step_id) AS num_trial_steps,\\n    SUM(CASE WHEN (ts.metadata->'parameters') IS NOT NULL THEN 1 ELSE 0 END) AS num_trial_steps_params\\nFROM\\n    user u\\n    INNER JOIN trial t ON u.user_id = t.user_id\\n    LEFT JOIN trial_step ts ON t.trial_id = ts.trial_id\\nGROUP BY\\n    u.user_id,\\n    t.trial_id;\\n```\\n&nbsp;\\n### Summary \\n\\nThe provided PostgreSQL query retrieves the username, user email and trial ID from the database. Also, it calculates the total number of trial steps and the count of trial steps that have parameters for each user and trial. The involvement of three tables in the query: user, trial and trial_step necessitates their join operations. The data is then grouped by user and trial ID.\"}, metadata={'finish_reason': 'stop'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chained Parameterized Prompts\n",
    "\n",
    "# prompt 2 uses prompt1's output. Prompt2 asks to translate into postgres\n",
    "await config.run(\"prompt2\", {}, inference_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': '',\n",
      " 'metadata': {'models': {'gpt-3.5-turbo': {'max_tokens': 3000,\n",
      "                                           'model': 'gpt-3.5-turbo',\n",
      "                                           'temperature': 1,\n",
      "                                           'top_p': 1},\n",
      "                         'gpt-4': {'max_tokens': 3000,\n",
      "                                   'model': 'gpt-4',\n",
      "                                   'system_prompt': 'You are an expert at SQL. '\n",
      "                                                    'You will output nicely '\n",
      "                                                    'formatted SQL code with '\n",
      "                                                    'labels on columns. You '\n",
      "                                                    'will provide a short 1-2 '\n",
      "                                                    'sentence summary on the '\n",
      "                                                    'code. Name columns as one '\n",
      "                                                    'word using underscore and '\n",
      "                                                    'lowercase. Format Output '\n",
      "                                                    'in markdown ### SQL Query '\n",
      "                                                    'code block with SQL Query '\n",
      "                                                    '&nbsp; ### Summary short '\n",
      "                                                    'summary on code',\n",
      "                                   'temperature': 1,\n",
      "                                   'top_p': 1}},\n",
      "              'parameters': {}},\n",
      " 'name': 'gpt4 as your data engineer',\n",
      " 'prompts': [{'input': 'Write me a {{sql_language}} query to get this final '\n",
      "                       'output: {{output_data}}. Use the tables relationships '\n",
      "                       'defined here: {{table_relationships}}.',\n",
      "              'metadata': {'model': {'name': 'gpt-3.5-turbo', 'settings': None},\n",
      "                           'parameters': {},\n",
      "                           'tags': None},\n",
      "              'name': 'prompt1'},\n",
      "             {'input': 'Translate the following into PostgreSQL code:\\n'\n",
      "                       ' {{prompt1.output}}',\n",
      "              'metadata': {'model': {'name': 'gpt-4',\n",
      "                                     'settings': {'max_tokens': 3000,\n",
      "                                                  'model': 'gpt-4',\n",
      "                                                  'temperature': 1,\n",
      "                                                  'top_p': 1}},\n",
      "                           'parameters': {},\n",
      "                           'tags': None},\n",
      "              'name': 'prompt2'},\n",
      "             {'input': 'How do transformers work?',\n",
      "              'metadata': {'model': 'gpt-3.5-turbo',\n",
      "                           'parameters': {},\n",
      "                           'remember_chat_context': False,\n",
      "                           'tags': None},\n",
      "              'name': 'prompt3'}],\n",
      " 'schema_version': 'latest'}\n"
     ]
    }
   ],
   "source": [
    "# CRUD API\n",
    "# Programatically add, delete, and update prompts in the config.\n",
    "import json\n",
    "from aiconfig_tools.AIConfigSettings import Prompt\n",
    "\n",
    "# Lets define a new prompt\n",
    "# Chat History defaults to True. Lets explicitly set it to false.\n",
    "prompt = Prompt(**{\"name\": \"prompt3\", \n",
    "                   \"input\": \"How do transformers work?\", \n",
    "                   \"metadata\": {\n",
    "                       \"model\": \"gpt-3.5-turbo\",\n",
    "                       \"remember_chat_context\": False,\n",
    "                }})\n",
    "\n",
    "config.add_prompt(\"prompt3\", prompt)\n",
    "\n",
    "# Exports aiconfig to disk.\n",
    "config.save(\"updated_aiconfig.json\")\n",
    "\n",
    "# Let's check out the new update config\n",
    "\n",
    "with open('updated_aiconfig.json', 'r') as f:\n",
    "    parsed = json.load(f)\n",
    "    pprint.pprint(parsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_tokens': 3000, 'top_p': 1, 'temperature': 1, 'model': 'gpt-3.5-turbo', 'messages': [{'content': 'How do transformers work?', 'role': 'user'}], 'stream': True}\n",
      "Transformers work on the principle of electromagnetic induction. They are made up of two coils, known as the primary and secondary coils, wound around a common iron core.\n",
      "\n",
      "When an alternating current (AC) passes through the primary coil, it generates a magnetic field around the iron core. This magnetic field constantly expands and contracts due to the alternating current.\n",
      "\n",
      "The changing magnetic field induces an alternating voltage in the secondary coil through electromagnetic induction. The voltage induced in the secondary coil depends on the turns ratio between the primary and secondary coils.\n",
      "\n",
      "The basic equation for understanding transformers is Vp/Vs = Np/Ns, where Vp and Vs represent the voltages on the primary and secondary coils respectively, and Np and Ns represent the number of turns in the primary and secondary coils respectively.\n",
      "\n",
      "Transformers are designed in such a way that the primary coil has fewer turns than the secondary coil in a step-up transformer, resulting in an increased voltage on the secondary side. Conversely, in a step-down transformer, the primary coil has more turns than the secondary coil, resulting in a reduced voltage on the secondary side.\n",
      "\n",
      "By changing the turns ratio, transformers can efficiently step up or step down voltages, allowing for transmission of electrical energy over long distances with minimal losses. They are widely used in power distribution networks, electrical appliances, and various electronic devices.\n"
     ]
    }
   ],
   "source": [
    "config2 = AIConfigRuntime.from_config(\"updated_aiconfig.json\")\n",
    "\n",
    "# Resolve\n",
    "print(await config2.resolve( \"prompt3\",{}, inference_options))\n",
    "\n",
    "# Run\n",
    "await config2.run(\"prompt3\", {})\n",
    "\n",
    "# Show output\n",
    "print(config2.get_output_text(\"prompt3\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
