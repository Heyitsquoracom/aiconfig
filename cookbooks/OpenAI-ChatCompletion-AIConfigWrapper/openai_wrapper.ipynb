{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easily create an AIConfig from existing Openai code\n",
    "1. Basic usage\n",
    "2. Load existing aiconfig and continue\n",
    "3. Capture function calling\n",
    "4. Use Client API\n",
    "5. Save to existing AIConfig (no JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AIConfig package\n",
    "!pip3 install python-aiconfig\n",
    "\n",
    "# Create ~/.env file with this line: export OPENAI_API_KEY=<your key here>\n",
    "# You can get your key from https://platform.openai.com/api-keys \n",
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pydantic/_internal/_fields.py:128: UserWarning: Field \"model_parsers\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai\n",
    "\n",
    "output_path = \"my-first-aiconfig.json\"\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=output_path,\n",
    ")\n",
    "\n",
    "def run_my_existing_openai_app(user_message: str):\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 1,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": user_message,\n",
    "                \"role\": \"user\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) # Creates a config saved to default path `aiconfig.json`\n",
    "    print(\"Chat Completion Response: \")\n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2024-01-04 22:11:33,787 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell me a joke about apples', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:33,789 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell me a joke about apples', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:33,790 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Tell me a joke about apples', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:33,790 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Tell me a joke about apples', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1704424292516630000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion Response: \n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_openai_app(\"Tell me a joke about apples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-first-aiconfig.json now represents your existing app:\n",
      "\n",
      "{\n",
      "  \"name\": \"\",\n",
      "  \"schema_version\": \"latest\",\n",
      "  \"metadata\": {\n",
      "    \"parameters\": {},\n",
      "    \"models\": {}\n",
      "  },\n",
      "  \"description\": \"\",\n",
      "  \"prompts\": [\n",
      "    {\n",
      "      \"name\": \"prompt_0\",\n",
      "      \"input\": \"Tell me a joke about apples\",\n",
      "      \"metadata\": {\n",
      "        \"model\": {\n",
      "          \"name\": \"gpt-3.5-turbo\",\n",
      "          \"settings\": {\n",
      "            \"model\": \"gpt-3.5-turbo\",\n",
      "            \"top_p\": 1,\n",
      "            \"max_tokens\": 3000,\n",
      "            \"temperature\": 1,\n",
      "            \"stream\": false\n",
      "          }\n",
      "        },\n",
      "        \"parameters\": {},\n",
      "        \"remember_chat_context\": true\n",
      "      },\n",
      "      \"outputs\": [\n",
      "        {\n",
      "          \"output_type\": \"execute_result\",\n",
      "          \"execution_count\": 0,\n",
      "          \"data\": {\n",
      "            \"content\": \"Why don't apples ever get lonely? \\n\\nBecause they all have a lot of core friends!\",\n",
      "            \"role\": \"assistant\"\n",
      "          },\n",
      "          \"metadata\": {\n",
      "            \"id\": \"chatcmpl-8dUuLsPM30SfOKklmGvkKKKj2cx0i\",\n",
      "            \"created\": 1704424293,\n",
      "            \"model\": \"gpt-3.5-turbo-0613\",\n",
      "            \"object\": \"chat.completion\",\n",
      "            \"usage\": {\n",
      "              \"completion_tokens\": 19,\n",
      "              \"prompt_tokens\": 13,\n",
      "              \"total_tokens\": 32\n",
      "            },\n",
      "            \"finish_reason\": \"stop\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"$schema\": \"https://json.schemastore.org/aiconfig-1.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"my-first-aiconfig.json now represents your existing app:\\n\")\n",
    "result = json.load(open(output_path))\n",
    "print(\n",
    "    json.dumps(result, indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoom in on the prompt:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"Tell me a joke about apples\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Why don't apples ever get lonely? \\n\\nBecause they all have a lot of core friends!\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8dUuLsPM30SfOKklmGvkKKKj2cx0i\",\n",
      "        \"created\": 1704424293,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 19,\n",
      "          \"prompt_tokens\": 13,\n",
      "          \"total_tokens\": 32\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"Zoom in on the prompt:\\n\")\n",
    "print(\n",
    "    json.dumps(result[\"prompts\"][0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Continue from existing aiconfig file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2024-01-04 22:11:34,484 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell a joke about apples in Shakespearian English.', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:34,485 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell a joke about apples in Shakespearian English.', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:34,486 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Tell a joke about apples in Shakespearian English.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:34,486 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Tell a joke about apples in Shakespearian English.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1704424292516630000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion Response: \n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_openai_app(\"Tell a joke about apples in Shakespearian English.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-first-aiconfig.json has your existing prompts:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"Tell me a joke about apples\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Why don't apples ever get lonely? \\n\\nBecause they all have a lot of core friends!\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8dUuLsPM30SfOKklmGvkKKKj2cx0i\",\n",
      "        \"created\": 1704424293,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 19,\n",
      "          \"prompt_tokens\": 13,\n",
      "          \"total_tokens\": 32\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"my-first-aiconfig.json has your existing prompts:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And your new prompt:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_1\",\n",
      "  \"input\": \"Tell a joke about apples in Shakespearian English.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Why did the apple turneth redeth?\\n\\nBecause 'twas afraid of becoming Juliet's fair roseth!\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8dUuMhp3JB7LOhtedqjeYKVrw5fkY\",\n",
      "        \"created\": 1704424294,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 23,\n",
      "          \"prompt_tokens\": 19,\n",
      "          \"total_tokens\": 42\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"And your new prompt:\\n\")\n",
    "print(\n",
    "    json.dumps(prompts[1], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Capture function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My existing app using function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Call Capture\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "output_path = \"my-function-calling-aiconfig.json\"\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=output_path,\n",
    ")\n",
    "\n",
    "\n",
    "def get_current_weather(location: str, unit: str) -> dict[str, Any]:\n",
    "    return { \"temperature\": 22, \"unit\": \"celsius\", \"description\": \"Sunny\" }\n",
    "\n",
    "\n",
    "def run_my_existing_weather_function_calling_app():\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo-0613\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"}],\n",
    "        \"functions\": [\n",
    "            {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) \n",
    "\n",
    "    function_call_response = get_current_weather(location=\"Boston\", unit=\"celsius\")\n",
    "    print(response)\n",
    "\n",
    "    completion_params = {\n",
    "      \"model\": \"gpt-3.5-turbo-0613\",\n",
    "      \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"},\n",
    "        {\"role\": \"assistant\", \"content\": 'null', \"function_call\": {\n",
    "              \"name\": \"get_current_weather\",\n",
    "              \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\"\n",
    "            }},\n",
    "        {\"role\": \"function\", \"name\": \"get_current_weather\", \"content\": str(function_call_response)}\n",
    "\n",
    "      ],\n",
    "      \"functions\": [\n",
    "        {\n",
    "          \"name\": \"get_current_weather\",\n",
    "          \"description\": \"Get the current weather in a given location\",\n",
    "          \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "              },\n",
    "              \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) \n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2024-01-04 22:11:35,410 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo-0613': 'gpt-3.5-turbo-0613', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:35,411 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:35,411 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:35,412 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1704424292516630000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8dUuMTRR32mrFFzNzhZBIbOIAnWPL', choices=[Choice(finish_reason='function_call', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n\"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None), logprobs=None)], created=1704424294, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=82, total_tokens=99))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2024-01-04 22:11:35,778 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo-0613': 'gpt-3.5-turbo-0613', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}, {'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, {'role': 'function', 'name': 'get_current_weather', 'content': \"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:35,779 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}, {'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, {'role': 'function', 'name': 'get_current_weather', 'content': \"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:35,780 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt_1', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[ExecuteResult(output_type='execute_result', execution_count=None, data='null', mime_type=None, metadata={'raw_response': {'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, 'role': 'assistant'})]), Prompt(name='prompt', input=PromptInput(attachments=None, data=None, role='function', name='get_current_weather', content=\"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"), metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:35,780 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt_1', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[ExecuteResult(output_type='execute_result', execution_count=None, data='null', mime_type=None, metadata={'raw_response': {'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, 'role': 'assistant'})]), Prompt(name='prompt', input=PromptInput(attachments=None, data=None, role='function', name='get_current_weather', content=\"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"), metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1704424292516630000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_weather_function_calling_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspect my-function-calling-aiconfig.json:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"What is the weather like in Boston?\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo-0613\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"functions\": [\n",
      "          {\n",
      "            \"name\": \"get_current_weather\",\n",
      "            \"description\": \"Get the current weather in a given location\",\n",
      "            \"parameters\": {\n",
      "              \"type\": \"object\",\n",
      "              \"properties\": {\n",
      "                \"location\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
      "                },\n",
      "                \"unit\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"enum\": [\n",
      "                    \"celsius\",\n",
      "                    \"fahrenheit\"\n",
      "                  ]\n",
      "                }\n",
      "              },\n",
      "              \"required\": [\n",
      "                \"location\"\n",
      "              ]\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"data\": \"null\",\n",
      "      \"metadata\": {\n",
      "        \"raw_response\": {\n",
      "          \"role\": \"assistant\",\n",
      "          \"content\": \"null\",\n",
      "          \"function_call\": {\n",
      "            \"name\": \"get_current_weather\",\n",
      "            \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\"\n",
      "          }\n",
      "        },\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Inspect my-function-calling-aiconfig.json:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Client API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai_client\n",
    "\n",
    "\n",
    "output_path = \"my-aiconfig-from-Client-API.json\"\n",
    "\n",
    "client = get_completion_create_wrapped_openai_client(output_path)\n",
    "\n",
    "def run_my_existing_client_api_app():\n",
    "    completion_params = {\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"top_p\": 1,\n",
    "                \"max_tokens\": 3000,\n",
    "                \"temperature\": 1,\n",
    "                \"stream\": False,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"content\": \"Compare and contrast bananas and cucumbers.\",\n",
    "                        \"role\": \"user\",\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "    response = client.chat.completions.create(**completion_params)\n",
    "    print(type(response))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2024-01-04 22:11:43,565 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Compare and contrast bananas and cucumbers.', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:43,566 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Compare and contrast bananas and cucumbers.', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:43,568 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Compare and contrast bananas and cucumbers.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:43,568 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Compare and contrast bananas and cucumbers.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1704424292516630000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_client_api_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspect my-aiconfig-from-Client-API.json:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"Compare and contrast bananas and cucumbers.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Bananas and cucumbers are both popular fruits, but they differ in various aspects. Let's compare and contrast them:\\n\\n1. Appearance: Bananas are elongated, curved fruits with a yellow color when ripe, while cucumbers are cylindrical with a green color throughout their ripening process. \\n\\n2. Taste: Bananas have a sweet taste with a creamy texture when ripe, while cucumbers have a crisp texture and a refreshing, mild flavor.\\n\\n3. Nutritional Value: Bananas are a rich source of potassium, dietary fiber, and vitamin C. They also contain significant amounts of vitamin B6 and manganese. Cucumbers, on the other hand, have comparatively lower nutritional value but are still a good source of hydration due to their high water content. They contain vitamins K, C, and A, as well as trace minerals like magnesium and potassium.\\n\\n4. Culinary Use: Bananas are commonly consumed as a snack or added to various dishes like smoothies, desserts, and breakfast cereals. They can also be used in baking. Cucumbers are often eaten fresh in salads, sandwiches, or as a side dish. They are also pickled or used for making tzatziki sauce.\\n\\n5. Ripening Process: Bananas are typically picked when green and firm and ripen off the tree. They undergo a transformation, turning yellow and becoming sweeter as they ripen. Cucumbers, in contrast, do not undergo a significant ripening process. They are typically consumed when green and immature, as they become bitter and develop seeds when overripe.\\n\\n6. Cultivation: Bananas are grown on large tropical plantations, primarily in countries like India, China, and the Philippines. They require warm climates and ample rainfall. Cucumbers, on the other hand, can be cultivated in a wider range of climates and are often grown in backyard gardens or commercial farms.\\n\\nOverall, while bananas and cucumbers are both fruits, they differ in terms of appearance, taste, nutritional value, culinary use, ripening process, and cultivation requirements.\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8dUuOtjcCrwWoeGhZxWy8Wr0YGp7y\",\n",
      "        \"created\": 1704424296,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 423,\n",
      "          \"prompt_tokens\": 16,\n",
      "          \"total_tokens\": 439\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(f\"Inspect {output_path}:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save to existing AIConfig (no JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai\n",
    "from aiconfig.Config import AIConfigRuntime\n",
    "\n",
    "existing_aiconfig = AIConfigRuntime.create()\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=existing_aiconfig,\n",
    ")\n",
    "\n",
    "def run_my_existing_openai_app(user_message: str):\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 1,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": user_message,\n",
    "                \"role\": \"user\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) # Creates a config saved to default path `aiconfig.json`\n",
    "    print(\"Chat Completion Response: \")\n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2024-01-04 22:11:44,890 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Are tomatoes fruits?', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:44,891 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Are tomatoes fruits?', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:44,891 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:11:44,892 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1704424292516630000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion Response: \n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "run_my_existing_openai_app(\"Are tomatoes fruits?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2024-01-04 22:13:00,362 callback.py:140: Callback called. event\n",
      ": name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'prompt_0', 'params': None, 'options': None, 'kwargs': {}} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:13:00,364 callback.py:140: Callback called. event\n",
      ": name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='prompt_0', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {}} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:13:00,365 callback.py:140: Callback called. event\n",
      ": name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='prompt_0', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[]), 'params': {}} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:13:00,368 callback.py:140: Callback called. event\n",
      ": name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 1, 'max_tokens': 3000, 'top_p': 1, 'stream': False, 'model': 'gpt-3.5-turbo', 'messages': [{'content': 'Are tomatoes fruits?', 'role': 'user'}]}} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:13:01,632 callback.py:140: Callback called. event\n",
      ": name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data='Yes, tomatoes are technically considered fruits. They develop from the ovary of a flowering plant and contain seeds. However, they are commonly referred to as vegetables in culinary contexts due to their savory taste and culinary usage.', mime_type=None, metadata={'raw_response': {'content': 'Yes, tomatoes are technically considered fruits. They develop from the ovary of a flowering plant and contain seeds. However, they are commonly referred to as vegetables in culinary contexts due to their savory taste and culinary usage.', 'role': 'assistant'}, 'id': 'chatcmpl-8dUvkZx2uDvlBot4vkNGItVkR0bfe', 'created': 1704424380, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 43, 'prompt_tokens': 11, 'total_tokens': 54}, 'finish_reason': 'stop', 'role': 'assistant'})]} ts_ns=1704424292516630000\n",
      "[INFO] 2024-01-04 22:13:01,633 callback.py:140: Callback called. event\n",
      ": name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data='Yes, tomatoes are technically considered fruits. They develop from the ovary of a flowering plant and contain seeds. However, they are commonly referred to as vegetables in culinary contexts due to their savory taste and culinary usage.', mime_type=None, metadata={'raw_response': {'content': 'Yes, tomatoes are technically considered fruits. They develop from the ovary of a flowering plant and contain seeds. However, they are commonly referred to as vegetables in culinary contexts due to their savory taste and culinary usage.', 'role': 'assistant'}, 'id': 'chatcmpl-8dUvkZx2uDvlBot4vkNGItVkR0bfe', 'created': 1704424380, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 43, 'prompt_tokens': 11, 'total_tokens': 54}, 'finish_reason': 'stop', 'role': 'assistant'})]} ts_ns=1704424292516630000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "Yes, tomatoes are technically considered fruits. They develop from the ovary of a flowering plant and contain seeds. However, they are commonly referred to as vegetables in culinary contexts due to their savory taste and culinary usage.\n"
     ]
    }
   ],
   "source": [
    "await existing_aiconfig.run(\"prompt_0\")\n",
    "\n",
    "print(\"Result:\")\n",
    "print(existing_aiconfig.get_output_text(\"prompt_0\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
