{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easily create an AIConfig from existing Openai code\n",
    "1. Basic usage\n",
    "2. Load existing aiconfig and continue\n",
    "3. Capture function calling\n",
    "4. Use Client API\n",
    "5. Save to existing AIConfig (no JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aiconfig/lib/python3.10/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_parsers\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/aiconfig/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai\n",
    "\n",
    "output_path = \"my-first-aiconfig.json\"\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=output_path,\n",
    ")\n",
    "\n",
    "def run_my_existing_openai_app(user_message: str):\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 1,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": user_message,\n",
    "                \"role\": \"user\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) # Creates a config saved to default path `aiconfig.json`\n",
    "    print(\"Chat Completion Response: \")\n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-11-29 15:46:49,369 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell me a joke about apples', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:49,370 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell me a joke about apples', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:49,371 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Tell me a joke about apples', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:49,372 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Tell me a joke about apples', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701290807824770000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion Response: \n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_openai_app(\"Tell me a joke about apples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-first-aiconfig.json now represents your existing app:\n",
      "\n",
      "{\n",
      "  \"name\": \"\",\n",
      "  \"schema_version\": \"latest\",\n",
      "  \"metadata\": {\n",
      "    \"parameters\": {},\n",
      "    \"models\": {}\n",
      "  },\n",
      "  \"description\": \"\",\n",
      "  \"prompts\": [\n",
      "    {\n",
      "      \"name\": \"prompt_0\",\n",
      "      \"input\": \"Tell me a joke about apples\",\n",
      "      \"metadata\": {\n",
      "        \"model\": {\n",
      "          \"name\": \"gpt-3.5-turbo\",\n",
      "          \"settings\": {\n",
      "            \"model\": \"gpt-3.5-turbo\",\n",
      "            \"top_p\": 1,\n",
      "            \"max_tokens\": 3000,\n",
      "            \"temperature\": 1,\n",
      "            \"stream\": false\n",
      "          }\n",
      "        },\n",
      "        \"parameters\": {},\n",
      "        \"remember_chat_context\": true\n",
      "      },\n",
      "      \"outputs\": [\n",
      "        {\n",
      "          \"output_type\": \"execute_result\",\n",
      "          \"execution_count\": 0,\n",
      "          \"data\": {\n",
      "            \"content\": \"Why did the apple go to school? \\n\\nBecause it wanted to become a \\\"smarty apple\\\"!\",\n",
      "            \"role\": \"assistant\"\n",
      "          },\n",
      "          \"metadata\": {\n",
      "            \"id\": \"chatcmpl-8QLkGrgJcAOf0cYbGLjYJAXkunEvB\",\n",
      "            \"created\": 1701290808,\n",
      "            \"model\": \"gpt-3.5-turbo-0613\",\n",
      "            \"object\": \"chat.completion\",\n",
      "            \"usage\": {\n",
      "              \"completion_tokens\": 20,\n",
      "              \"prompt_tokens\": 13,\n",
      "              \"total_tokens\": 33\n",
      "            },\n",
      "            \"finish_reason\": \"stop\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"prompt_1\",\n",
      "      \"input\": \"Tell a joke about apples in Shakespearian English.\",\n",
      "      \"metadata\": {\n",
      "        \"model\": {\n",
      "          \"name\": \"gpt-3.5-turbo\",\n",
      "          \"settings\": {\n",
      "            \"model\": \"gpt-3.5-turbo\",\n",
      "            \"top_p\": 1,\n",
      "            \"max_tokens\": 3000,\n",
      "            \"temperature\": 1,\n",
      "            \"stream\": false\n",
      "          }\n",
      "        },\n",
      "        \"parameters\": {},\n",
      "        \"remember_chat_context\": true\n",
      "      },\n",
      "      \"outputs\": [\n",
      "        {\n",
      "          \"output_type\": \"execute_result\",\n",
      "          \"execution_count\": 0,\n",
      "          \"data\": {\n",
      "            \"content\": \"Why did the apple attend the play? \\n\\nBecause it wanted to be the core of attention, and take a bite out of the spotlight, whilst avoiding the tragedie of being pomme-struck by the actors!\",\n",
      "            \"role\": \"assistant\"\n",
      "          },\n",
      "          \"metadata\": {\n",
      "            \"id\": \"chatcmpl-8QLk3AHPZEb5zAfaQEpgzoZPgmbzb\",\n",
      "            \"created\": 1701290795,\n",
      "            \"model\": \"gpt-3.5-turbo-0613\",\n",
      "            \"object\": \"chat.completion\",\n",
      "            \"usage\": {\n",
      "              \"completion_tokens\": 43,\n",
      "              \"prompt_tokens\": 19,\n",
      "              \"total_tokens\": 62\n",
      "            },\n",
      "            \"finish_reason\": \"stop\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"my-first-aiconfig.json now represents your existing app:\\n\")\n",
    "result = json.load(open(output_path))\n",
    "print(\n",
    "    json.dumps(result, indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoom in on the prompt:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"Tell me a joke about apples\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Why did the apple go to school? \\n\\nBecause it wanted to become a \\\"smarty apple\\\"!\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8QLkGrgJcAOf0cYbGLjYJAXkunEvB\",\n",
      "        \"created\": 1701290808,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 20,\n",
      "          \"prompt_tokens\": 13,\n",
      "          \"total_tokens\": 33\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"Zoom in on the prompt:\\n\")\n",
    "print(\n",
    "    json.dumps(result[\"prompts\"][0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Continue from existing aiconfig file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-11-29 15:46:50,135 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell a joke about apples in Shakespearian English.', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:50,137 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell a joke about apples in Shakespearian English.', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:50,138 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Tell a joke about apples in Shakespearian English.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:50,139 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Tell a joke about apples in Shakespearian English.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701290807824770000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion Response: \n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_openai_app(\"Tell a joke about apples in Shakespearian English.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-first-aiconfig.json has your existing prompts:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"Tell me a joke about apples\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Why did the apple go to school? \\n\\nBecause it wanted to become a \\\"smarty apple\\\"!\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8QLkGrgJcAOf0cYbGLjYJAXkunEvB\",\n",
      "        \"created\": 1701290808,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 20,\n",
      "          \"prompt_tokens\": 13,\n",
      "          \"total_tokens\": 33\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"my-first-aiconfig.json has your existing prompts:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And your new prompt:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_1\",\n",
      "  \"input\": \"Tell a joke about apples in Shakespearian English.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Why did the apple make for a dainty queen, thou asketh?\\n\\nForsooth, 'twas her core that held her thine esteem.\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8QLkHuPf9PFh6tsDFilqgWiljvb1c\",\n",
      "        \"created\": 1701290809,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 31,\n",
      "          \"prompt_tokens\": 19,\n",
      "          \"total_tokens\": 50\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"And your new prompt:\\n\")\n",
    "print(\n",
    "    json.dumps(prompts[1], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Capture function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My existing app using function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Call Capture\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "output_path = \"my-function-calling-aiconfig.json\"\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=output_path,\n",
    ")\n",
    "\n",
    "\n",
    "def get_current_weather(location: str, unit: str) -> dict[str, Any]:\n",
    "    return { \"temperature\": 22, \"unit\": \"celsius\", \"description\": \"Sunny\" }\n",
    "\n",
    "\n",
    "def run_my_existing_weather_function_calling_app():\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo-0613\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"}],\n",
    "        \"functions\": [\n",
    "            {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) \n",
    "\n",
    "    function_call_response = get_current_weather(location=\"Boston\", unit=\"celsius\")\n",
    "    print(response)\n",
    "\n",
    "    completion_params = {\n",
    "      \"model\": \"gpt-3.5-turbo-0613\",\n",
    "      \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"},\n",
    "        {\"role\": \"assistant\", \"content\": 'null', \"function_call\": {\n",
    "              \"name\": \"get_current_weather\",\n",
    "              \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\"\n",
    "            }},\n",
    "        {\"role\": \"function\", \"name\": \"get_current_weather\", \"content\": str(function_call_response)}\n",
    "\n",
    "      ],\n",
    "      \"functions\": [\n",
    "        {\n",
    "          \"name\": \"get_current_weather\",\n",
    "          \"description\": \"Get the current weather in a given location\",\n",
    "          \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "              },\n",
    "              \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) \n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-11-29 15:46:51,020 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo-0613': 'gpt-3.5-turbo-0613', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:51,022 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:51,023 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:51,024 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701290807824770000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8QLkIImNPeu4nzBf7v2O7zFZMXeAN', choices=[Choice(finish_reason='function_call', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1701290810, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=82, total_tokens=100))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-11-29 15:46:51,672 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo-0613': 'gpt-3.5-turbo-0613', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}, {'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, {'role': 'function', 'name': 'get_current_weather', 'content': \"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:51,673 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}, {'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, {'role': 'function', 'name': 'get_current_weather', 'content': \"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:51,674 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt_1', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[ExecuteResult(output_type='execute_result', execution_count=None, data={'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, mime_type=None, metadata={})]), Prompt(name='prompt', input=PromptInput(data=None, role='function', name='get_current_weather', content=\"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"), metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:51,675 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt_1', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[ExecuteResult(output_type='execute_result', execution_count=None, data={'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, mime_type=None, metadata={})]), Prompt(name='prompt', input=PromptInput(data=None, role='function', name='get_current_weather', content=\"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"), metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701290807824770000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_weather_function_calling_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspect my-function-calling-aiconfig.json:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"What is the weather like in Boston?\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo-0613\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"functions\": [\n",
      "          {\n",
      "            \"name\": \"get_current_weather\",\n",
      "            \"description\": \"Get the current weather in a given location\",\n",
      "            \"parameters\": {\n",
      "              \"type\": \"object\",\n",
      "              \"properties\": {\n",
      "                \"location\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
      "                },\n",
      "                \"unit\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"enum\": [\n",
      "                    \"celsius\",\n",
      "                    \"fahrenheit\"\n",
      "                  ]\n",
      "                }\n",
      "              },\n",
      "              \"required\": [\n",
      "                \"location\"\n",
      "              ]\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"data\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"null\",\n",
      "        \"function_call\": {\n",
      "          \"name\": \"get_current_weather\",\n",
      "          \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"metadata\": {}\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Inspect my-function-calling-aiconfig.json:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Client API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai_client\n",
    "\n",
    "\n",
    "output_path = \"my-aiconfig-from-Client-API.json\"\n",
    "\n",
    "client = get_completion_create_wrapped_openai_client(output_path)\n",
    "\n",
    "def run_my_existing_client_api_app():\n",
    "    completion_params = {\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"top_p\": 1,\n",
    "                \"max_tokens\": 3000,\n",
    "                \"temperature\": 1,\n",
    "                \"stream\": False,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"content\": \"Compare and contrast bananas and cucumbers.\",\n",
    "                        \"role\": \"user\",\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "    response = client.chat.completions.create(**completion_params)\n",
    "    print(type(response))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-11-29 15:46:58,980 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Compare and contrast bananas and cucumbers.', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:58,982 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Compare and contrast bananas and cucumbers.', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:58,984 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Compare and contrast bananas and cucumbers.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:46:58,985 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Compare and contrast bananas and cucumbers.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701290807824770000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_client_api_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspect my-aiconfig-from-Client-API.json:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"Compare and contrast bananas and cucumbers.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Bananas and cucumbers are both popular fruits, but they differ significantly in terms of taste, nutritional value, and usage. \\n\\nIn terms of taste, bananas have a sweet flavor with a creamy and soft texture. On the other hand, cucumbers have a mild, crisp, and refreshing taste. Bananas are renowned for their natural sugars, while cucumbers are known for their high water content, which contributes to their refreshing taste and crunchy texture.\\n\\nRegarding nutritional value, bananas are rich in potassium, dietary fiber, vitamin C, and vitamin B6. They offer a good source of energy and are often consumed for their ability to help regulate blood pressure and promote heart health. Additionally, bananas are a great choice for athletes as they provide a boost of energy due to their high carbohydrate content. Cucumbers, on the other hand, are low in calories and fat and are primarily composed of water. They are excellent for hydration and provide small amounts of vitamins K and C.\\n\\nIn terms of usage, bananas are versatile and commonly eaten raw as a quick snack, in smoothies, as an ingredient in desserts, or even added to savory dishes. They can also be used in baking to replace eggs or as a natural sweetener due to their concentrated sweetness. Cucumbers, on the other hand, are typically consumed raw in salads, sandwiches, or as a standalone snack. They are often pickled to create sour pickles enjoyed in various cuisines worldwide.\\n\\nTo summarize, bananas and cucumbers differ significantly in taste, nutritional value, and usage. Bananas are sweet and creamy, packed with potassium and vitamin C, and can be eaten raw or used in various recipes. Cucumbers are refreshing and mild, with high water content, low calorie count, and are primarily consumed raw in salads or pickled.\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8QLkJrmbatxOR2JiSkHhKeNlNm6mM\",\n",
      "        \"created\": 1701290811,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 370,\n",
      "          \"prompt_tokens\": 16,\n",
      "          \"total_tokens\": 386\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(f\"Inspect {output_path}:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save to existing AIConfig (no JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai\n",
    "from aiconfig.Config import AIConfigRuntime\n",
    "\n",
    "existing_aiconfig = AIConfigRuntime.create()\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=existing_aiconfig,\n",
    ")\n",
    "\n",
    "def run_my_existing_openai_app(user_message: str):\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 1,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": user_message,\n",
    "                \"role\": \"user\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) # Creates a config saved to default path `aiconfig.json`\n",
    "    print(\"Chat Completion Response: \")\n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-11-29 15:47:00,587 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Are tomatoes fruits?', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:47:00,590 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Are tomatoes fruits?', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:47:00,592 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:47:00,594 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701290807824770000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion Response: \n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "run_my_existing_openai_app(\"Are tomatoes fruits?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-11-29 15:47:00,609 callback.py:140: Callback called. event\n",
      ": name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'prompt_0', 'params': None, 'options': None, 'kwargs': {}} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:47:00,612 callback.py:140: Callback called. event\n",
      ": name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='prompt_0', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'Yes, tomatoes are considered fruits. Although they are commonly used as vegetables in cooking, they are technically classified as fruits due to their botanical definition. Fruits are the edible parts of a flowering plant that contain seeds, and tomatoes meet this criteria.', 'role': 'assistant'}, mime_type=None, metadata={'id': 'chatcmpl-8QLkRIgOiHY2KCsdIwJf8Kk2yAYTt', 'created': 1701290819, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 49, 'prompt_tokens': 11, 'total_tokens': 60}, 'finish_reason': 'stop'})]), 'options': None, 'parameters': {}} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:47:00,613 callback.py:140: Callback called. event\n",
      ": name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='prompt_0', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'Yes, tomatoes are considered fruits. Although they are commonly used as vegetables in cooking, they are technically classified as fruits due to their botanical definition. Fruits are the edible parts of a flowering plant that contain seeds, and tomatoes meet this criteria.', 'role': 'assistant'}, mime_type=None, metadata={'id': 'chatcmpl-8QLkRIgOiHY2KCsdIwJf8Kk2yAYTt', 'created': 1701290819, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 49, 'prompt_tokens': 11, 'total_tokens': 60}, 'finish_reason': 'stop'})]), 'params': {}} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:47:00,620 callback.py:140: Callback called. event\n",
      ": name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'max_tokens': 3000, 'top_p': 1, 'temperature': 1, 'stream': False, 'model': 'gpt-3.5-turbo', 'messages': [{'content': 'Are tomatoes fruits?', 'role': 'user'}, {'content': 'Yes, tomatoes are considered fruits. Although they are commonly used as vegetables in cooking, they are technically classified as fruits due to their botanical definition. Fruits are the edible parts of a flowering plant that contain seeds, and tomatoes meet this criteria.', 'role': 'assistant'}]}} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:47:02,630 callback.py:140: Callback called. event\n",
      ": name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': \"Yes, tomatoes are fruits. According to the botanical definition, a fruit is the mature ovary of a flowering plant that contains seeds. Tomatoes meet this definition because they develop from the ovary of the tomato plant's flower and contain seeds. However, they are often referred to and used as vegetables in culinary contexts.\", 'role': 'assistant'}, mime_type=None, metadata={'id': 'chatcmpl-8QLkTmLElhFKIaYjCsYWuafcYn2oi', 'created': 1701290821, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 64, 'prompt_tokens': 64, 'total_tokens': 128}, 'finish_reason': 'stop'})]} ts_ns=1701290807824770000\n",
      "[INFO] 2023-11-29 15:47:02,633 callback.py:140: Callback called. event\n",
      ": name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': \"Yes, tomatoes are fruits. According to the botanical definition, a fruit is the mature ovary of a flowering plant that contains seeds. Tomatoes meet this definition because they develop from the ovary of the tomato plant's flower and contain seeds. However, they are often referred to and used as vegetables in culinary contexts.\", 'role': 'assistant'}, mime_type=None, metadata={'id': 'chatcmpl-8QLkTmLElhFKIaYjCsYWuafcYn2oi', 'created': 1701290821, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 64, 'prompt_tokens': 64, 'total_tokens': 128}, 'finish_reason': 'stop'})]} ts_ns=1701290807824770000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "Yes, tomatoes are fruits. According to the botanical definition, a fruit is the mature ovary of a flowering plant that contains seeds. Tomatoes meet this definition because they develop from the ovary of the tomato plant's flower and contain seeds. However, they are often referred to and used as vegetables in culinary contexts.\n"
     ]
    }
   ],
   "source": [
    "await existing_aiconfig.run(\"prompt_0\")\n",
    "\n",
    "print(\"Result:\")\n",
    "print(existing_aiconfig.get_output_text(\"prompt_0\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
