{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easily create an AIConfig from existing Openai code\n",
    "1. Basic usage\n",
    "2. Load existing aiconfig and continue\n",
    "3. Capture function calling\n",
    "4. Use Client API\n",
    "5. Save to existing AIConfig (no JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ~/.env file with this line: export OPENAI_API_KEY=<your key here>\n",
    "# You can get your key from https://platform.openai.com/api-keys \n",
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aiconfig/lib/python3.10/site-packages/pydantic/_internal/_fields.py:128: UserWarning: Field \"model_parsers\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/aiconfig/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai\n",
    "\n",
    "output_path = \"my-first-aiconfig.json\"\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=output_path,\n",
    ")\n",
    "\n",
    "def run_my_existing_openai_app(user_message: str):\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 1,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": user_message,\n",
    "                \"role\": \"user\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) # Creates a config saved to default path `aiconfig.json`\n",
    "    print(\"Chat Completion Response: \")\n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-01 14:03:43,767 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell me a joke about apples', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:43,768 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell me a joke about apples', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:43,769 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Tell me a joke about apples', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:43,770 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Tell me a joke about apples', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion Response: \n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_openai_app(\"Tell me a joke about apples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-first-aiconfig.json now represents your existing app:\n",
      "\n",
      "{\n",
      "  \"name\": \"\",\n",
      "  \"schema_version\": \"latest\",\n",
      "  \"metadata\": {\n",
      "    \"parameters\": {},\n",
      "    \"models\": {}\n",
      "  },\n",
      "  \"description\": \"\",\n",
      "  \"prompts\": [\n",
      "    {\n",
      "      \"name\": \"prompt_0\",\n",
      "      \"input\": \"Tell me a joke about apples\",\n",
      "      \"metadata\": {\n",
      "        \"model\": {\n",
      "          \"name\": \"gpt-3.5-turbo\",\n",
      "          \"settings\": {\n",
      "            \"model\": \"gpt-3.5-turbo\",\n",
      "            \"top_p\": 1,\n",
      "            \"max_tokens\": 3000,\n",
      "            \"temperature\": 1,\n",
      "            \"stream\": false\n",
      "          }\n",
      "        },\n",
      "        \"parameters\": {},\n",
      "        \"remember_chat_context\": true\n",
      "      },\n",
      "      \"outputs\": [\n",
      "        {\n",
      "          \"output_type\": \"execute_result\",\n",
      "          \"execution_count\": 0,\n",
      "          \"data\": {\n",
      "            \"content\": \"Why did the apple go to therapy?\\n\\nBecause it had a bad peel-ing!\",\n",
      "            \"role\": \"assistant\"\n",
      "          },\n",
      "          \"metadata\": {\n",
      "            \"id\": \"chatcmpl-8R35bxxwnkQy4YronOeJWLLQ5fkKP\",\n",
      "            \"created\": 1701457423,\n",
      "            \"model\": \"gpt-3.5-turbo-0613\",\n",
      "            \"object\": \"chat.completion\",\n",
      "            \"usage\": {\n",
      "              \"completion_tokens\": 16,\n",
      "              \"prompt_tokens\": 13,\n",
      "              \"total_tokens\": 29\n",
      "            },\n",
      "            \"finish_reason\": \"stop\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"prompt_1\",\n",
      "      \"input\": \"Tell a joke about apples in Shakespearian English.\",\n",
      "      \"metadata\": {\n",
      "        \"model\": {\n",
      "          \"name\": \"gpt-3.5-turbo\",\n",
      "          \"settings\": {\n",
      "            \"model\": \"gpt-3.5-turbo\",\n",
      "            \"top_p\": 1,\n",
      "            \"max_tokens\": 3000,\n",
      "            \"temperature\": 1,\n",
      "            \"stream\": false\n",
      "          }\n",
      "        },\n",
      "        \"parameters\": {},\n",
      "        \"remember_chat_context\": true\n",
      "      },\n",
      "      \"outputs\": [\n",
      "        {\n",
      "          \"output_type\": \"execute_result\",\n",
      "          \"execution_count\": 0,\n",
      "          \"data\": {\n",
      "            \"content\": \"Why did the apple become a noble knight in fair Verona? \\n\\nBecause it wished to 'tis the season, and transform from fruit to Rome-dreaming tree, but alas, it could only become a fallen pair, appealing to thee!\",\n",
      "            \"role\": \"assistant\"\n",
      "          },\n",
      "          \"metadata\": {\n",
      "            \"id\": \"chatcmpl-8R34kiMdly4hq5g1KrdVschroQWgV\",\n",
      "            \"created\": 1701457370,\n",
      "            \"model\": \"gpt-3.5-turbo-0613\",\n",
      "            \"object\": \"chat.completion\",\n",
      "            \"usage\": {\n",
      "              \"completion_tokens\": 50,\n",
      "              \"prompt_tokens\": 19,\n",
      "              \"total_tokens\": 69\n",
      "            },\n",
      "            \"finish_reason\": \"stop\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"my-first-aiconfig.json now represents your existing app:\\n\")\n",
    "result = json.load(open(output_path))\n",
    "print(\n",
    "    json.dumps(result, indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoom in on the prompt:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"Tell me a joke about apples\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Why did the apple go to therapy?\\n\\nBecause it had a bad peel-ing!\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8R35bxxwnkQy4YronOeJWLLQ5fkKP\",\n",
      "        \"created\": 1701457423,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 16,\n",
      "          \"prompt_tokens\": 13,\n",
      "          \"total_tokens\": 29\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"Zoom in on the prompt:\\n\")\n",
    "print(\n",
    "    json.dumps(result[\"prompts\"][0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Continue from existing aiconfig file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-01 14:03:44,950 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell a joke about apples in Shakespearian English.', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:44,953 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell a joke about apples in Shakespearian English.', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:44,954 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Tell a joke about apples in Shakespearian English.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:44,956 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Tell a joke about apples in Shakespearian English.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion Response: \n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_openai_app(\"Tell a joke about apples in Shakespearian English.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-first-aiconfig.json has your existing prompts:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"Tell me a joke about apples\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Why did the apple go to therapy?\\n\\nBecause it had a bad peel-ing!\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8R35bxxwnkQy4YronOeJWLLQ5fkKP\",\n",
      "        \"created\": 1701457423,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 16,\n",
      "          \"prompt_tokens\": 13,\n",
      "          \"total_tokens\": 29\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"my-first-aiconfig.json has your existing prompts:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And your new prompt:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_1\",\n",
      "  \"input\": \"Tell a joke about apples in Shakespearian English.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Why did Romeo have an apple tree in his garden?\\n\\nForsooth, he wished to have a fruit with his Juliet, but verily, he missed the peach and ended up with a Granny Smith!\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8R35bYMBANDKFbmTP0vN2y4K4p9j5\",\n",
      "        \"created\": 1701457423,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 41,\n",
      "          \"prompt_tokens\": 19,\n",
      "          \"total_tokens\": 60\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"And your new prompt:\\n\")\n",
    "print(\n",
    "    json.dumps(prompts[1], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Capture function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My existing app using function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Call Capture\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "output_path = \"my-function-calling-aiconfig.json\"\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=output_path,\n",
    ")\n",
    "\n",
    "\n",
    "def get_current_weather(location: str, unit: str) -> dict[str, Any]:\n",
    "    return { \"temperature\": 22, \"unit\": \"celsius\", \"description\": \"Sunny\" }\n",
    "\n",
    "\n",
    "def run_my_existing_weather_function_calling_app():\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo-0613\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"}],\n",
    "        \"functions\": [\n",
    "            {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) \n",
    "\n",
    "    function_call_response = get_current_weather(location=\"Boston\", unit=\"celsius\")\n",
    "    print(response)\n",
    "\n",
    "    completion_params = {\n",
    "      \"model\": \"gpt-3.5-turbo-0613\",\n",
    "      \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"},\n",
    "        {\"role\": \"assistant\", \"content\": 'null', \"function_call\": {\n",
    "              \"name\": \"get_current_weather\",\n",
    "              \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\"\n",
    "            }},\n",
    "        {\"role\": \"function\", \"name\": \"get_current_weather\", \"content\": str(function_call_response)}\n",
    "\n",
    "      ],\n",
    "      \"functions\": [\n",
    "        {\n",
    "          \"name\": \"get_current_weather\",\n",
    "          \"description\": \"Get the current weather in a given location\",\n",
    "          \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "              },\n",
    "              \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) \n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-01 14:03:45,823 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo-0613': 'gpt-3.5-turbo-0613', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:45,825 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:45,827 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:45,828 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8R35d8LM16rpKZPjstq56h2FqbyoM', choices=[Choice(finish_reason='function_call', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1701457425, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=82, total_tokens=100))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-01 14:03:46,462 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo-0613': 'gpt-3.5-turbo-0613', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}, {'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, {'role': 'function', 'name': 'get_current_weather', 'content': \"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:46,463 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}, {'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, {'role': 'function', 'name': 'get_current_weather', 'content': \"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:46,465 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt_1', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[ExecuteResult(output_type='execute_result', execution_count=None, data={'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, mime_type=None, metadata={})]), Prompt(name='prompt', input=PromptInput(data=None, role='function', name='get_current_weather', content=\"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"), metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:46,466 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt_1', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[ExecuteResult(output_type='execute_result', execution_count=None, data={'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, mime_type=None, metadata={})]), Prompt(name='prompt', input=PromptInput(data=None, role='function', name='get_current_weather', content=\"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"), metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_weather_function_calling_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspect my-function-calling-aiconfig.json:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"What is the weather like in Boston?\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo-0613\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"functions\": [\n",
      "          {\n",
      "            \"name\": \"get_current_weather\",\n",
      "            \"description\": \"Get the current weather in a given location\",\n",
      "            \"parameters\": {\n",
      "              \"type\": \"object\",\n",
      "              \"properties\": {\n",
      "                \"location\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
      "                },\n",
      "                \"unit\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"enum\": [\n",
      "                    \"celsius\",\n",
      "                    \"fahrenheit\"\n",
      "                  ]\n",
      "                }\n",
      "              },\n",
      "              \"required\": [\n",
      "                \"location\"\n",
      "              ]\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"data\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"null\",\n",
      "        \"function_call\": {\n",
      "          \"name\": \"get_current_weather\",\n",
      "          \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"metadata\": {}\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Inspect my-function-calling-aiconfig.json:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Client API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai_client\n",
    "\n",
    "\n",
    "output_path = \"my-aiconfig-from-Client-API.json\"\n",
    "\n",
    "client = get_completion_create_wrapped_openai_client(output_path)\n",
    "\n",
    "def run_my_existing_client_api_app():\n",
    "    completion_params = {\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"top_p\": 1,\n",
    "                \"max_tokens\": 3000,\n",
    "                \"temperature\": 1,\n",
    "                \"stream\": False,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"content\": \"Compare and contrast bananas and cucumbers.\",\n",
    "                        \"role\": \"user\",\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "    response = client.chat.completions.create(**completion_params)\n",
    "    print(type(response))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-01 14:03:56,300 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Compare and contrast bananas and cucumbers.', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:56,302 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Compare and contrast bananas and cucumbers.', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:56,303 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Compare and contrast bananas and cucumbers.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:56,304 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Compare and contrast bananas and cucumbers.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_client_api_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspect my-aiconfig-from-Client-API.json:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"Compare and contrast bananas and cucumbers.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Bananas and cucumbers are both popular fruits that are consumed worldwide. However, they have several contrasting characteristics as well.\\n\\nPhysical Appearance:\\n- Bananas: They are elongated fruits with a curved shape, typically yellow in color when ripe, and have a thick, peeling skin.\\n- Cucumbers: They are cylindrical in shape, usually longer than bananas, and have a smooth, dark green skin.\\n\\nTaste and Texture:\\n- Bananas: They have a sweet and creamy taste with a soft, mushy texture when ripe, making them easy to chew and digest.\\n- Cucumbers: They have a refreshingly mild and slightly bitter taste with a crispy, crunchy texture. They are known for their high water content.\\n\\nNutritional Profile:\\n- Bananas: They are a good source of potassium, vitamin C, vitamin B6, and dietary fiber. They also contain natural sugars, making them a good energy source.\\n- Cucumbers: They are low in calories and rich in water, making them hydrating. They also provide some vitamin K, vitamin C, and dietary fiber, although in smaller amounts compared to bananas.\\n\\nCulinary Uses:\\n- Bananas: They are consumed as a snack, added to smoothies, used in baking (for desserts like banana bread), and can be frozen for later use. They are also commonly used in making ice cream and milkshakes.\\n- Cucumbers: They are often used fresh in salads, sandwiches, and pickles. Cucumbers can also be blended into juices or added to infused water for flavor.\\n\\nCultural Significance:\\n- Bananas: They are staple fruits in many tropical countries and are an important part of various cuisines. Bananas are widely associated with health benefits, energy, and are a popular breakfast choice.\\n- Cucumbers: They are extensively used in salads and pickling traditions in cultures around the world, adding a refreshing and crunchy element to meals.\\n\\nOverall, while bananas and cucumbers share some similarities as fruits, their distinctive characteristics, taste, and culinary uses set them apart from each other.\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8R35ecL6ekpb6Ca5fBxOtiyYplORM\",\n",
      "        \"created\": 1701457426,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 430,\n",
      "          \"prompt_tokens\": 16,\n",
      "          \"total_tokens\": 446\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(f\"Inspect {output_path}:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save to existing AIConfig (no JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai\n",
    "from aiconfig.Config import AIConfigRuntime\n",
    "\n",
    "existing_aiconfig = AIConfigRuntime.create()\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=existing_aiconfig,\n",
    ")\n",
    "\n",
    "def run_my_existing_openai_app(user_message: str):\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 1,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": user_message,\n",
    "                \"role\": \"user\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) # Creates a config saved to default path `aiconfig.json`\n",
    "    print(\"Chat Completion Response: \")\n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-01 14:03:57,426 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Are tomatoes fruits?', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:57,428 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Are tomatoes fruits?', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:57,430 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:57,432 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion Response: \n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "run_my_existing_openai_app(\"Are tomatoes fruits?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-01 14:03:57,440 callback.py:140: Callback called. event\n",
      ": name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'prompt_0', 'params': None, 'options': None, 'kwargs': {}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:57,442 callback.py:140: Callback called. event\n",
      ": name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='prompt_0', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'Yes, tomatoes are classified as fruits. Although they are commonly used as a vegetable in cooking, botanically they are the fruit of the tomato plant.', 'role': 'assistant'}, mime_type=None, metadata={'id': 'chatcmpl-8R35ohinQTGI4uoWXn61NJIH4pYgO', 'created': 1701457436, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 31, 'prompt_tokens': 11, 'total_tokens': 42}, 'finish_reason': 'stop'})]), 'options': None, 'parameters': {}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:57,443 callback.py:140: Callback called. event\n",
      ": name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='prompt_0', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'Yes, tomatoes are classified as fruits. Although they are commonly used as a vegetable in cooking, botanically they are the fruit of the tomato plant.', 'role': 'assistant'}, mime_type=None, metadata={'id': 'chatcmpl-8R35ohinQTGI4uoWXn61NJIH4pYgO', 'created': 1701457436, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 31, 'prompt_tokens': 11, 'total_tokens': 42}, 'finish_reason': 'stop'})]), 'params': {}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:57,446 callback.py:140: Callback called. event\n",
      ": name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'model': 'gpt-3.5-turbo', 'max_tokens': 3000, 'top_p': 1, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Are tomatoes fruits?', 'role': 'user'}, {'content': 'Yes, tomatoes are classified as fruits. Although they are commonly used as a vegetable in cooking, botanically they are the fruit of the tomato plant.', 'role': 'assistant'}]}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:58,852 callback.py:140: Callback called. event\n",
      ": name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'To be more specific, tomatoes are technically classified as a berry. They develop from the ovary of a flower and contain seeds, which makes them a fruit. However, they are often treated and referred to as a vegetable due to their culinary usage.', 'role': 'assistant'}, mime_type=None, metadata={'id': 'chatcmpl-8R35p1LoxPLVjJFQWBHWjO962fz9q', 'created': 1701457437, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 50, 'prompt_tokens': 46, 'total_tokens': 96}, 'finish_reason': 'stop'})]} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:58,854 callback.py:140: Callback called. event\n",
      ": name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'To be more specific, tomatoes are technically classified as a berry. They develop from the ovary of a flower and contain seeds, which makes them a fruit. However, they are often treated and referred to as a vegetable due to their culinary usage.', 'role': 'assistant'}, mime_type=None, metadata={'id': 'chatcmpl-8R35p1LoxPLVjJFQWBHWjO962fz9q', 'created': 1701457437, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 50, 'prompt_tokens': 46, 'total_tokens': 96}, 'finish_reason': 'stop'})]} ts_ns=1701457422515626000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "To be more specific, tomatoes are technically classified as a berry. They develop from the ovary of a flower and contain seeds, which makes them a fruit. However, they are often treated and referred to as a vegetable due to their culinary usage.\n"
     ]
    }
   ],
   "source": [
    "await existing_aiconfig.run(\"prompt_0\")\n",
    "\n",
    "print(\"Result:\")\n",
    "print(existing_aiconfig.get_output_text(\"prompt_0\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
