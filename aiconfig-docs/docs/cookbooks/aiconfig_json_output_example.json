{
  "name": "AiConfig Demo",
  "schema_version": "latest",
  "metadata": {
    "models": {
      "gpt-4": {
        "model": "gpt-4",
        "top_p": 1,
        "max_tokens": 4096,
        "presence_penalty": 0,
        "frequency_penalty": 0
      }
    },
    "parameters": {
      "ARTICLE": ""
    }
  },
  "prompts": [
    {
      "name": "ARTICLE",
      "input": "Abstract\nComputational fact-checking has gained a lot\nof traction in the machine learning and natural\nlanguage processing communities. A plethora\nof solutions have been developed, but methods\nwhich leverage both structured and unstructured information to detect misinformation are\nof particular relevance. In this paper, we tackle\nthe FEVEROUS (Fact Extraction and VERification Over Unstructured and Structured information) challenge which consists of an open\nsource baseline system together with a benchmark dataset containing 87,026 verified claims.\nWe extend this baseline model by improving\nthe evidence retrieval module yielding the best\nevidence F1 score among the competitors in\nthe challenge leaderboard while obtaining an\noverall FEVEROUS score of 0.20 (5th best\nranked system).\n1 Introduction\nThe volume of potentially misleading and false\nclaims has surged with the increasing usage of the\nweb and social media. No barriers exist for publishing information which make anyone capable\nof diffusing false or biased claims while reaching\nlarge audiences with ease (Baptista and Gradim,\n2020). One approach of dealing with this ordeal\nis computational fact-checking (Wu et al., 2014),\nwhere the automation of the verification pipeline or\nparts of it is flourishing due to advances in natural\nlanguage processing (Nakov et al., 2021; Saeed\nand Papotti, 2021). Along these lines, several\ndatasets and fact-evaluation algorithms have been\nproposed (Kotonya and Toni, 2020).\nIn this paper, we report on our effort in tackling\nthe FEVEROUS challenge (Aly et al., 2021). The\nprovided dataset consists in a set of textual claims\nverified against evidence retrieved from a corpus\nof English Wikipedia pages. The claims are labeled as supported, refuted or NEI (Not Enough\nInformation). Evidence can be unstructured (such\nas sentences) or structured (such as general tables,\ninfoboxes, lists, etc.). The task is to return the right\nlabel with the correct evidence. The baseline model\nis divided in two main parts: an evidence retrieval\npart and a verdict prediction part. The evaluation\nis performed through the so-called FEVEROUS\nscore which is computed considering both the correct retrieval of the evidence and the correct label\npredictions. In this paper, we propose an enhanced\nversion of this baseline model that focuses on the\nretrieval component through a re-ranking process\nof pages, resulting in a more precise model.\nIn the remainder of this paper, we first describe\nbriefly the challenge task and the supplied data,\nand we detail our extension (Section 2). We then\nprovide experimental results obtained on the development dataset and we discuss observations on\nanalyzed errors (Section 3). We conclude with a\ndiscussion of other research directions that can be\napplied to improve results for the FEVEROUS task\n(Section 4).\n2 Method\nWe begin by reviewing the given baseline, and then\npropose an extension to it that improved the precision and recall of the page-retrieval module in\nexchange for more computation time.\n2.1 FEVEROUS Baseline\nIn FEVEROUS (Aly et al., 2021), the aim is to\nfind out the veracity of a claim c. This is done\nby: (i) acquiring a set of evidence E which could\ncontain sentences extracted from a Wikipedia page,\nor cell(s) from a Wikipedia table, and (ii) predicting\na label y ∈ {Supports, Refutes, NEI}.\nThe proposed baseline is simple yet competitive (Aly et al., 2021). For (i), a combination of\nentity-matching and TF-IDF scoring are used to\nidentify the most prominent Wikipedia pages (Chen\net al., 2017). k pages are selected by matching entities extracted from the claim to Wikipedia pages.\n109\nIf needed, remaining pages are identified using\nTF-IDF matching between the claim and the introductory sentence of the page. Given the extracted\nWikipedia pages, sentences are scored through a\ndot product with the claim in the TF-IDF space,\nwhere the top l sentences are retrieved. Similarly,\nthe top q tables are extracted where the TF-IDF\nvector of the table title is used to represent a table.\nThe tables are then linearized, pre-processed to respect the input-size limit of the classifier (Oguz\net al., 2020), and then used, alongside the claim, to\nfine-tune a RoBERTa model (Liu et al., 2020) on a\nbinary token classification task.\nFor (ii), given the retrieved evidence, the final\nverdict is predicted using a RoBERTa model with\na sequence-classification layer which is fed with\nsequentially concatenated claim and evidences as\ninput. The model has been trained on a set of\nlabelled claims (71,291 samples) with their associated evidence.\n2.2 Proposed Extension\nIt is clear that to enhance the system, evidence retrieval should be a top priority as identifying the\ncorrect evidence is crucial for the verdict predictor\nto function properly. We focus on enhancing the\nidentification of Wikipedia pages by utilizing advances in the information retrieval (IR) community\nwhere neural ranking models have been proposed\nfor better data retrieval (Mitra et al., 2016; Hui\net al., 2018).\nA simple IR pipeline comprises a two-stage reranking process where: (a) first, a large number of\ndocuments to a given query are retrieved from a corpus using a standard mechanism such as TF-IDF or\nBM25; (b) second, the documents are scored and reranked using a more computationally-demanding\nmethod. Given that neural ranking methods have\nshown success in the IR community (Guo et al.,\n2019), we used this method as part of our extension.\nFor (a), we use the current page-retriever based\non entity-matching and TF-IDF to retrieve a higher\nnumber of pages. For (b), the re-ranker model\nprovides a score si\nindicating how relevant a page\npi\nis to an input claim c. The re-ranker is based\non a pre-trained BERT model (Devlin et al., 2019)\nthat is fine-tuned on the passage re-ranking task of\nthe MS MACRO dataset (Nguyen et al., 2016) to\nminimize the binary cross-entropy loss:\nL = −\nX\ni∈I+\nlog(si) −\nX\ni∈I−\nlog(1 − si) (1)\nwhere I\n+ and I\n− are the set of indices of the relevant and non-relevant pages respectively in the\ntop-1,000 MS MACRO documents retrieved with\nBM25 (Nogueira and Cho, 2019). We designate a\npage re-ranker model as a function P R(m) which\ntake a set of relevant pages for a claim, which are\nusually scored by a less-computationally demanding method such as TF-IDF to limit the set of candidates, scores them, and outputs the top m pages.\n3 Experiments\nModel. We rely on the cross-encoder model\nfine-tuned on the MS MARCO Passage Ranking\ntask (Reimers and Gurevych, 2019) provided on the\nHugging Face model hub. We feed the claim with\nevery extracted page into the re-ranker model to\nobtain scores used to re-rank the respective pages.\nSettings. We set k = 150 and m = 5\nwhere 150 pages are first extracted through entitymatching+TF-IDF, scored with the re-ranker\nmodel, and then the top-5 pages for each claim are\nextracted. The remainder of the pipeline remains\nintact (l = 5, q = 3). We designate this pipeline\nby BLpage(150) → P R(5) → tf idf(5, 3). Our\ncode can be found at https://gitlab.eurecom.\nfr/saeedm1/eurecom-fever.\nResults. We see improvement with the page reranker as the coverage of documents has increased\ncompared to the baseline. Hence, for k = 5, the\nretriever without the re-ranker achieves a document\ncoverage of 69% on the dev set, while the addition\nof the re-ranker enhances the coverage to around\n79%, which, in turn, improves the FEVEROUS\nand F1 scores, compared to the initial baseline (Table 1, in bold). While the page re-ranker improves\nthe document coverage, we do not observe pronounced improvements on the system as a whole.\nEven with a better page retriever, an increase in\nFEVEROUS and F1 scores requires improvements\nalso in the sentence and cell evidence retrievers.\nAlthough more time-demanding, the re-ranker\ngives more subtle results than an entity-matching\napproach (Aly et al., 2021). For example, given\nthe following excerpt of a claim: “Family Guy\nis an American animated sitcom that features\nfive main voice actors [...] and has appeared in\n22 (out of 349) episodes [...] that has appeared\n110\nFS LA EP ER E-F1\nBL(5, 5, 3) 0.19 0.54 0.12 0.29 0.17\nBL(5, 5, 3)full 0.186 0.533 0.119 0.289 0.168\nBLpage(50) → P R(5) → tf idf(5, 3) 0.129 0.468 0.120 0.201 0.151\nBLpage(150) → P R(5) → tf idf(5, 3) 0.218 0.548 0.145 0.339 0.203\nBLpage(150) → P R(5) → BM25(5, 3) 0.205 0.550 0.127 0.321 0.182\nBLpage(150) → P R(5) → SR(5) → tf idftable(3) 0.184 0.501 0.130 0.283 0.179\nTable 1: Results on the dev set showing the FEVEROUS Score (FS), the Label Accuracy (LA), the Evidence\nPrecision (EP), the Evidence Recall (ER), and the Evidence F1-score (E-F1) of the different system variants.\nin 90 episodes.”, one can directly see that the\nretrieved pages should be related to the series\n“Family Guy”. The baseline fails to predict the\ncorrect page Family Guy, and instead matches\nwith entities such as Guy and American, and\nWikipedia pages for numbers such as 90 and 22.\nAdditionally, some pages retrieved with TF-IDF do\nnot relate to the claim at hand: John Manwood\n(MP) and John Manwood. The page re-ranker,\non the other hand, manages to get the correct page\nin the top-5 predictions, where all other predictions\nare related: List of Family Guy guest\nstars, List of Family Guy episodes,\nBlue Harvest (an episode from the TV series),\nList of Family Guy cast members,\nand Family Guy. Finally, we observe that the\nentity-matching process is brittle and fails to match\nthe sub-string “Angela Santomero\" to the page\nAngela C. Santomero as it only performs\nexact string matching.\nThere are some cases where entitymatching+TF-IDF outperformed the re-ranker:\nsome of those are cases where the Wikipedia page\ncontent is small and does not bring much benefit on\na semantic level and this is where TF-IDF works\nbetter. We observe that we tend to miss the correct\npage when there are several pages who share\nsimilar semantics. For example, given the claim:\n“Seven notable animated television series,\nincluding Super Why!, a children’s educational\nshow created by Angela C. Santomero and\nSamantha Freeman Alpert, Phineas and Ferb\nand WordGirl, were released in September\n2007.”, the page re-ranker retrieves TV shows\nthat are produced Angela C. Santomero).\nHowever, the correct page Phineas and Ferb\ndoes not appear in the top-5 predictions, and\nk FEVEROUS Score Time(mins)\n200 0.216 140\n300 0.224 210\n500 0.219 345\nTable 2: Results on the dev set showing the FEVEROUS Score and the recorded time for the re-ranking\nprocessing for varying values of k.\nother pages take the lead, whereas the baseline\ncan identify the correct page by entity matching,\nalthough its predictions are not as coherent as\nthose of the page re-ranker.\nOther Attempts. We have experimented with\nvarying the number of extracted pages k. We measure also the time taken for re-ranking. Table 2\nshows the results. We observe that increasing the\nnumber of pages to extract does not always increase\nthe FEVEROUS score, as more candidate pages act\nas distractors to the other modules in the pipeline.\nWe have attempted to perform other extensions\nto the system that we describe below (Table 1).\nFirstly, we specified the re-ranking system to\nextract less pages (50), but it worsened the scores.\nThis configuration is defined as BLpage(50) →\nP R(5) → tf idf(5, 3).\nFurthermore, we applied the same re-ranking\napproach at the sentence level. After obtaining\n150 pages from the page re-ranker, we continue\nto retrieve all sentences from every page and\nre-rank them using the same passage re-ranking\nmodel (Nguyen et al., 2016), (BLpage(150) →\nP R(5) → SR(5) → tf idftable(3)). However, despite great outputs of page re-ranker, we could not\nobtain better results from sentences re-ranker than\nTF-IDF.\n111\nRegarding how relevant sentences and tables\nare chosen as evidence, apart from TF-IDF + Cosine Similarity, we also experimented with the\nOkapi BM25 scoring function (Robertson et al.,\n1995). This is applied after the pages are re-ranked,\n(BLpage(150) → P R(5) → BM25(5, 3)). Surprisingly, although BM25 is generally preferred for\ndocument retrieval, in our case, it did not lead to\nbetter results compared to TF-IDF. One possible\ncause might lie in text preprocessing, as we did not\nfully explore different combinations of preprocessing functions.\nLastly, we attempted to improve the verdict predictor by (i) fine-tuning the verdict classifier on\nthe full training dataset (BL(5, 5, 3)full) and by\n(ii) utilizing other pre-trained models that are either larger or were pre-finetuned on a NLI dataset.\nHowever, we did not observe significant improvements from them since their performance on the\ndev set was either on par or slightly worse than\nthe baseline model signaling that the focus enhancing of the second part of the system requires more\nsignificant changes.\n4 Conclusion and Future Directions\nIn this work, we have proposed the inclusion of a\nneural re-ranker model as a refinement step after\nstandard methods such as TF-IDF. While being\nmore intensive on the computational side, we do\nsee improvements on the document-retrieval side\nwhere results are more sound. There are of course\nmore directions that are worth exploring to improve\nthe results further.\nSentence retrieval could be improved by incorporating a pre-trained neural network that performs\nsemantic matching between the claims and the sentences. One instance of such models is where the\ntext sequences are encoded, then passed through an\nalignment layer that computes aligned representations of the encoded input sequences, followed by\na matching layer that performs the semantic matching (Nie et al., 2018). Such models have been\napplied on the FEVER dataset (Thorne et al., 2018)\nand have been shown to outperform the TF-IDF\napproach (Nie et al., 2018).\nCell retrieval could be enhanced by utilizing\npre-trained models over tables that outperform pretrained models over text (Herzig et al., 2021). Several systems that exploit table structure have been\nproposed for the task of fact-checking a claim\nover a table. However, not all of them can be\nused in every setting as each system holds different attributes and dimensions that need to be\ncomprehended to better integrate them in certain\ntasks (Saeed and Papotti, 2021). For example,\nsome systems such as SCRUTINIZER (Karagiannis et al., 2020) are dependent on the table-schema\nand would not benefit in the FEVEROUS scenario where tables have varying schema. Yet, other\nsystems such as TAPAS (Herzig et al., 2020) are\nschema-independent and can be fine-tuned on the\navailable FEVEROUS dataset to provide a score\nfor a given table, thus acting as a table ranker module. Some of these systems can be even directly\ntrained on the data, to get domain-specific models.\nOnce the tables have been identified, a classifier\ncan be trained on top of models that output cell representations of a table, such as TaBERT (Yin et al.,\n2020) and TURL (Deng et al., 2020), to extract the\nkey cells for verdict prediction. Also, fine-tuning\nthe re-ranker models on the given data is a viable\napproach. Finally, more sophisticated entity matching algorithms could have been explored to avoid\nthe “exact match” issues that we observed with the\nbaseline’s entity matching (C. et al., 2018).",
      "metadata": {
        "model": {
          "name": "gpt-4",
          "settings": {
            "temperature": 1,
            "system_prompt": "You will generate increasingly concise, entity-dense summaries from articles.\n\nGuidelines:\n- The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n- The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n- Missing entities can appear anywhere in the new summary.\n- Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n\nRemember, use the exact same number of words for each summary."
          }
        },
        "parameters": {},
        "remember_chat_context": true
      }
    },
    {
      "name": "identify_missing_entities",
      "input": "Identify 1-3 informative Entities (\"; \" delimited) from {{ARTICLE.input}} which are missing from the previously generated summary.",
      "metadata": {
        "model": {
          "name": "gpt-4",
          "settings": {
            "temperature": 0.25,
            "system_prompt": "A Missing Entity is:\n- Relevant: to the main story.\n- Specific: descriptive yet concise (5 words or fewer).\n- Novel: not in the previous summary.\n- Faithful: present in the Article.\n- Anywhere: located anywhere in the Article."
          }
        },
        "parameters": {},
        "remember_chat_context": false
      }
    },
    {
      "name": "generate_dense_summary",
      "input": "Write a new, denser summary of identical length which covers every entity and detail from {{ARTICLE.output}} \n and {{identify_missing_entities.output}}",
      "metadata": {
        "model": {
          "name": "gpt-4",
          "settings": {
            "temperature": 0.5,
            "system_prompt": "You will generate increasingly concise, entity-dense summaries of the above Article.\n\nGuidelines:\n- The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n- The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n- Missing entities can appear anywhere in the new summary.\n- Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n\nRemember, use the exact same number of words for each summary."
          }
        },
        "parameters": {},
        "remember_chat_context": false
      }
    },
    {
      "name": "cell_4",
      "input": "Repeat the steps from {{identify_missing_entities.input}} and {{generate_dense_summary.input}} 5 times.",
      "metadata": {
        "model": {
          "name": "gpt-4",
          "settings": {
            "temperature": 0,
            "system_prompt": "Guidelines:\n- The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n- The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n- Missing entities can appear anywhere in the new summary.\n- Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n\nRemember, use the exact same number of words for each summary.\n\nAnswer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\"."
          }
        },
        "parameters": {},
        "remember_chat_context": false
      }
    }
  ]
}
